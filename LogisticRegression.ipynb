{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìñ Introduction to Logistic Regression\n",
    "\n",
    "**Logistic Regression** is a classification model used to predict **a binary outcome** (0 or 1).  \n",
    "Unlike **Linear Regression**, which predicts a continuous value, it applies a **sigmoid function** to obtain a **probability**.\n",
    "\n",
    "### üîç Difference between Linear and Logistic Regression:\n",
    "| **Linear Regression** | **Logistic Regression** |\n",
    "|----------------------|----------------------|\n",
    "| Predicts a continuous value | Predicts a probability (0 to 1) |\n",
    "| Uses identity function $f(x) = x$ | Uses the sigmoid function $f(x) = \\frac{1}{1 + e^{-x}}$ |\n",
    "| Measures error with MSE | Uses Log-Loss (Cross-Entropy) |\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Mathematics Behind Logistic Regression\n",
    "\n",
    "### üîπ 1. Sigmoid Function  \n",
    "The **sigmoid function** transforms a linear output into a **probability**:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "where $z = X\\beta$ (dot product of features and coefficients).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. Cost Function (Log Loss)  \n",
    "Instead of MSE, Logistic Regression uses the **log-loss function**:\n",
    "\n",
    "$$\n",
    "J(\\beta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "This **penalizes incorrect predictions** that are close to 0 or 1.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 3. Optimization with Gradient Descent  \n",
    "To minimize the log-loss, we update **$\\beta$** using Gradient Descent:\n",
    "\n",
    "$$\n",
    "\\beta := \\beta - \\alpha \\frac{\\partial J}{\\partial \\beta}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üì¶ Importing Required Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Loading and Exploring the Dataset\n",
    "\n",
    "For this notebook, we will use the **Breast Cancer dataset**, a common dataset used for binary classification.  \n",
    "It contains **features extracted from cell nuclei** and a **binary target variable** (0 = Malignant, 1 = Benign).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows √ó 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  Target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset (Breast Cancer)\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df[\"Target\"] = data.target  # 0 = Malignant, 1 = Benign\n",
    "\n",
    "# Display first five rows\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Checking for Missing Values\n",
    "\n",
    "Before training our model, we need to ensure that there are **no missing values** in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Missing values per column:\n",
      " mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "Target                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"üîç Missing values per column:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Feature Scaling and Splitting the Data\n",
    "\n",
    "To improve model performance:\n",
    "1. We **normalize the features** using `StandardScaler()`.\n",
    "2. We **split the dataset** into **training (80%) and test (20%)** sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data prepared: Train = (455, 30), Test = (114, 30)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=[\"Target\"])\n",
    "y = df[\"Target\"]\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"‚úÖ Data prepared: Train = {X_train.shape}, Test = {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Training the Logistic Regression Model\n",
    "\n",
    "We now **train our Logistic Regression model** on the training set and evaluate its performance.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Evaluation Metrics for Classification\n",
    "\n",
    "Since Logistic Regression is a **classification model**, we evaluate its performance using the following key metrics:\n",
    "\n",
    "### üîπ 1. Accuracy  \n",
    "Measures the proportion of **correct predictions** over all predictions:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Samples}}\n",
    "$$\n",
    "\n",
    "‚úÖ Good when **classes are balanced**.  \n",
    "‚ö† Can be **misleading for imbalanced datasets**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. Precision  \n",
    "Measures the proportion of **true positive predictions** among all predicted positives:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    "\n",
    "‚úÖ Useful when **False Positives are costly** (e.g., spam detection).  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 3. Recall (Sensitivity)  \n",
    "Measures the proportion of **correctly identified positives** among all actual positives:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "\n",
    "‚úÖ Useful when **False Negatives are costly** (e.g., cancer detection).  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 4. F1-Score  \n",
    "Harmonic mean of Precision and Recall, balancing both metrics:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "‚úÖ Ideal when we need a **tradeoff** between Precision and Recall.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 5. Confusion Matrix  \n",
    "A matrix that **summarizes predictions vs. actual classes**:\n",
    "\n",
    "|  | Predicted Positive | Predicted Negative |\n",
    "|--|------------------|------------------|\n",
    "| **Actual Positive** | True Positives (TP) | False Negatives (FN) |\n",
    "| **Actual Negative** | False Positives (FP) | True Negatives (TN) |\n",
    "\n",
    "- **False Positives (FP)**: Incorrectly classified as positive.  \n",
    "- **False Negatives (FN)**: Incorrectly classified as negative.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 6. ROC Curve & AUC Score  \n",
    "The **Receiver Operating Characteristic (ROC) curve** shows the tradeoff between **True Positive Rate (TPR) and False Positive Rate (FPR)**:\n",
    "\n",
    "- **True Positive Rate (TPR) = Recall**:\n",
    "\n",
    "$$\n",
    "TPR = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- **False Positive Rate (FPR)**:\n",
    "\n",
    "$$\n",
    "FPR = \\frac{FP}{FP + TN}\n",
    "$$\n",
    "\n",
    "- **AUC (Area Under Curve)**: Measures the overall ability of the model to distinguish between classes.\n",
    "\n",
    "‚úÖ AUC **closer to 1** ‚Üí Better model performance.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model Performance:\n",
      "Accuracy  : 0.9737\n",
      "Precision : 0.9722\n",
      "Recall    : 0.9859\n",
      "F1-Score  : 0.9790\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"üìä Model Performance:\")\n",
    "print(f\"Accuracy  : {accuracy:.4f}\")\n",
    "print(f\"Precision : {precision:.4f}\")\n",
    "print(f\"Recall    : {recall:.4f}\")\n",
    "print(f\"F1-Score  : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìâ Visualizing the Confusion Matrix\n",
    "\n",
    "The **confusion matrix** helps understand how well the model differentiates between classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Malignant\", \"Benign\"], yticklabels=[\"Malignant\", \"Benign\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ Regularization: Ridge (L2) and Lasso (L1)\n",
    "\n",
    "To **prevent overfitting**, we apply **regularization techniques** that add a **penalty term** to the cost function, discouraging large coefficients.\n",
    "\n",
    "### üîπ Why Regularization?  \n",
    "- **High variance models** tend to overfit training data.\n",
    "- **Regularization constrains the magnitude of coefficients**, making the model **simpler and more generalizable**.\n",
    "- Helps when **features are highly correlated (multicollinearity)**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ 1. L2 Regularization (Ridge Regression)  \n",
    "Ridge Regression **adds an L2 penalty** to the cost function:\n",
    "\n",
    "$$\n",
    "J(\\beta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "- **Effect:** Shrinks coefficients **towards zero** but **does not remove them**.\n",
    "- **Helps when features are correlated** by distributing weights.\n",
    "- **Large $\\lambda$ ‚Üí Stronger penalty** ‚Üí More shrinkage.\n",
    "\n",
    "‚úÖ **Good for high-dimensional data where all features might be relevant.**  \n",
    "\n",
    "---\n",
    "\n",
    "### üìñ 2. L1 Regularization (Lasso Regression)  \n",
    "Lasso Regression **adds an L1 penalty**, which **can force some coefficients to exactly zero**:\n",
    "\n",
    "$$\n",
    "J(\\beta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "$$\n",
    "\n",
    "- **Effect:** Shrinks some coefficients to zero, effectively performing **feature selection**.\n",
    "- **Encourages sparsity**, making the model easier to interpret.\n",
    "- **Larger $\\lambda$ ‚Üí More features set to zero**.\n",
    "\n",
    "‚úÖ **Useful when we suspect that only a few features are important.**\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Comparison of Ridge and Lasso\n",
    "\n",
    "| **Method**  | **Penalty Term** | **Effect on Coefficients** | **Feature Selection?** | **Best Use Case** |\n",
    "|------------|---------------|--------------------|-----------------|----------------|\n",
    "| **Ridge (L2)** | $ \\lambda \\sum \\beta_j^2$ | Shrinks coefficients but keeps all | ‚ùå No | When features are correlated |\n",
    "| **Lasso (L1)** | $ \\lambda \\sum \\beta_j $ | Shrinks and removes some coefficients | ‚úÖ Yes | When only a few features matter |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ridge and Lasso models trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize models with regularization\n",
    "model_ridge = LogisticRegression(penalty=\"l2\", C=1.0)  # Ridge\n",
    "model_lasso = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=1.0)  # Lasso\n",
    "\n",
    "# Train models\n",
    "model_ridge.fit(X_train, y_train)\n",
    "model_lasso.fit(X_train, y_train)\n",
    "\n",
    "print(f\"‚úÖ Ridge and Lasso models trained successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Interview Questions\n",
    "\n",
    "1Ô∏è‚É£ **What is the difference between Logistic and Linear Regression?**  \n",
    "   - Linear Regression predicts **continuous values**, while Logistic Regression predicts **probabilities**.  \n",
    "\n",
    "2Ô∏è‚É£ **Why does Logistic Regression use Log-Loss instead of MSE?**  \n",
    "   - Log-Loss **penalizes wrong predictions more effectively**, especially for values close to 0 or 1.  \n",
    "\n",
    "3Ô∏è‚É£ **How do you interpret the coefficients in Logistic Regression?**  \n",
    "   - A coefficient **$\\beta_i$** represents the **log-odds ratio** of $x_i$ on the target.  \n",
    "\n",
    "4Ô∏è‚É£ **Why do we need regularization in Logistic Regression?**  \n",
    "   - To **prevent overfitting** and handle **multicollinearity**.  \n",
    "\n",
    "5Ô∏è‚É£ **How do you handle an imbalanced dataset in Logistic Regression?**  \n",
    "   - Use **class weights**, **oversampling/undersampling**, or **SMOTE** techniques.  \n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
