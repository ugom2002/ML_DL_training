{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 Neural Networks (Basics) - A Complete Guide\n",
    "\n",
    "This notebook provides a **detailed breakdown** of fundamental neural network concepts, including:  \n",
    "✔ **Perceptron & Multi-layer Perceptron (MLP)**  \n",
    "✔ **Activation Functions & Their Derivatives**  \n",
    "✔ **Forward & Backpropagation with Detailed Math**  \n",
    "✔ **Gradient Descent for Neural Networks**  \n",
    "✔ **Implementation in PyTorch & Keras**\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 1. Introduction to Neural Networks\n",
    "\n",
    "Neural Networks are inspired by the **human brain** and consist of **layers of neurons** that transform input data through weighted connections.  \n",
    "\n",
    "### 🔹 Why Use Neural Networks?\n",
    "✔ **Can model complex relationships** between inputs and outputs.  \n",
    "✔ **Can approximate any function (Universal Approximation Theorem)**.  \n",
    "✔ **Used in deep learning applications like NLP, Vision, and Reinforcement Learning**.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 2. The Perceptron: The Basic Building Block\n",
    "\n",
    "### 🔹 1. The Perceptron Model  \n",
    "A perceptron is a **single-layer neural network** that performs **binary classification**:\n",
    "\n",
    "$$\n",
    "y = f(W \\cdot X + b)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **$X$** = Input vector\n",
    "- **$W$** = Weights\n",
    "- **$b$** = Bias\n",
    "- **$f$** = Activation function\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Perceptron Learning Rule\n",
    "\n",
    "1. Initialize weights randomly.  \n",
    "2. Compute the **weighted sum** of inputs:  \n",
    "\n",
    "   $$\n",
    "   z = W \\cdot X + b\n",
    "   $$\n",
    "\n",
    "3. Apply an **activation function** (e.g., step function):\n",
    "\n",
    "   $$\n",
    "   y = \\text{sign}(z)\n",
    "   $$\n",
    "\n",
    "4. **Update weights** using the perceptron update rule:\n",
    "\n",
    "   $$\n",
    "   W := W + \\alpha (y_{\\text{true}} - y_{\\text{pred}}) X\n",
    "   $$\n",
    "\n",
    "5. Repeat until convergence.\n",
    "\n",
    "✅ **Limitations**:  \n",
    "- Can only solve **linearly separable** problems (e.g., AND, OR).\n",
    "- **Cannot solve XOR** → Requires Multi-Layer Perceptrons (MLPs).\n",
    "\n",
    "---\n",
    "\n",
    "# 📖 3. Multi-Layer Perceptron (MLP)\n",
    "\n",
    "A **Multi-Layer Perceptron (MLP)** consists of:\n",
    "- **Input Layer**\n",
    "- **Hidden Layers** (with non-linear activations)\n",
    "- **Output Layer**\n",
    "\n",
    "Each neuron in layer **$l$** receives input **$a^{(l-1)}$** from the previous layer:\n",
    "\n",
    "$$\n",
    "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{(l)} = f(z^{(l)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **$W^{(l)}$** = Weights of layer **$l$**  \n",
    "- **$b^{(l)}$** = Bias  \n",
    "- **$f$** = Activation function  \n",
    "\n",
    "✅ **MLPs can learn complex patterns, including XOR**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 4. Activation Functions & Their Derivatives\n",
    "\n",
    "### 🔹 1. Sigmoid Function\n",
    "Used in binary classification:\n",
    "\n",
    "$$\n",
    "f(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Derivative:\n",
    "\n",
    "$$\n",
    "f'(z) = f(z) (1 - f(z))\n",
    "$$\n",
    "\n",
    "✅ **Smooth, differentiable**  \n",
    "⚠ **Suffers from vanishing gradients**  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. ReLU (Rectified Linear Unit)\n",
    "Most popular in deep networks:\n",
    "\n",
    "$$\n",
    "f(z) = \\max(0, z)\n",
    "$$\n",
    "\n",
    "Derivative:\n",
    "\n",
    "$$\n",
    "f'(z) =\n",
    "\\begin{cases}\n",
    "1, & z > 0 \\\\\n",
    "0, & z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "✅ **Faster convergence**  \n",
    "⚠ **Can cause dead neurons (ReLU dying problem)**  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 3. Softmax Function\n",
    "Used in multi-class classification:\n",
    "\n",
    "$$\n",
    "f(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial z_i} = f(z_i) (1 - f(z_i))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 5. Backpropagation: Training Neural Networks\n",
    "\n",
    "To train an MLP, we use **Backpropagation**, which updates weights based on the **chain rule of calculus**.\n",
    "\n",
    "### 🔹 1. Compute Loss Function\n",
    "For **binary classification**, we use **Binary Cross-Entropy**:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "For **multi-class classification**, we use **Categorical Cross-Entropy**:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\sum_{i=1}^{m} \\sum_{j=1}^{k} y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Compute Gradients Using Chain Rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W} = \\frac{\\partial J}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 6. Implementing Neural Networks in PyTorch and Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ugo11\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 📦 Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.0323\n",
      "Epoch 20/100, Loss: 0.0440\n",
      "Epoch 30/100, Loss: 0.3302\n",
      "Epoch 40/100, Loss: 0.0009\n",
      "Epoch 50/100, Loss: 0.0017\n",
      "Epoch 60/100, Loss: 0.0009\n",
      "Epoch 70/100, Loss: 0.0869\n",
      "Epoch 80/100, Loss: 0.1213\n",
      "Epoch 90/100, Loss: 0.0073\n",
      "Epoch 100/100, Loss: 0.3324\n",
      "📊 Accuracy on the test set: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# ✅ 1. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features (4 attributes)\n",
    "y = iris.target  # Labels (0, 1, 2)\n",
    "\n",
    "# ✅ 2. Preprocess the data\n",
    "scaler = StandardScaler()  # Standardize the features\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)  # Multi-class classification\n",
    "\n",
    "# ✅ 3. Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ 4. Create DataLoader for batch training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# ✅ 5. Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(4, 16)  # 4 input features -> 16 hidden neurons\n",
    "        self.layer2 = nn.Linear(16, 8)  # 16 -> 8 hidden neurons\n",
    "        self.layer3 = nn.Linear(8, 3)   # 8 -> 3 output classes (softmax not needed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)  # No softmax here since CrossEntropyLoss handles it\n",
    "        return x\n",
    "\n",
    "# ✅ 6. Initialize model, loss function, and optimizer\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ✅ 7. Train the model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ✅ 8. Evaluate the model\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        _, predicted = torch.max(y_pred, 1)  # Select the class with the highest probability\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"📊 Accuracy on the test set: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 7. Implementing Neural Networks in Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ugo11\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.7481 - loss: 0.8462 - val_accuracy: 0.8000 - val_loss: 0.7987\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7136 - loss: 0.8334 - val_accuracy: 0.8667 - val_loss: 0.7588\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7523 - loss: 0.7562 - val_accuracy: 0.9333 - val_loss: 0.7203\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7569 - loss: 0.7510 - val_accuracy: 0.9333 - val_loss: 0.6844\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7692 - loss: 0.7342 - val_accuracy: 0.9333 - val_loss: 0.6509\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7729 - loss: 0.6823 - val_accuracy: 0.9333 - val_loss: 0.6183\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8164 - loss: 0.6308 - val_accuracy: 0.9000 - val_loss: 0.5884\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8045 - loss: 0.6237 - val_accuracy: 0.9000 - val_loss: 0.5616\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8349 - loss: 0.5349 - val_accuracy: 0.9000 - val_loss: 0.5350\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8225 - loss: 0.5503 - val_accuracy: 0.9000 - val_loss: 0.5098\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8112 - loss: 0.5667 - val_accuracy: 0.9000 - val_loss: 0.4864\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8165 - loss: 0.5392 - val_accuracy: 0.9000 - val_loss: 0.4648\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8039 - loss: 0.5195 - val_accuracy: 0.9000 - val_loss: 0.4444\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7451 - loss: 0.5758 - val_accuracy: 0.9000 - val_loss: 0.4246\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8148 - loss: 0.5117 - val_accuracy: 0.9000 - val_loss: 0.4050\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8349 - loss: 0.4644 - val_accuracy: 0.9000 - val_loss: 0.3855\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8422 - loss: 0.4852 - val_accuracy: 0.9000 - val_loss: 0.3673\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8636 - loss: 0.4322 - val_accuracy: 0.9000 - val_loss: 0.3510\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8429 - loss: 0.4292 - val_accuracy: 0.9000 - val_loss: 0.3366\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8715 - loss: 0.3963 - val_accuracy: 0.9000 - val_loss: 0.3219\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8278 - loss: 0.4429 - val_accuracy: 0.9000 - val_loss: 0.3082\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8706 - loss: 0.3628 - val_accuracy: 0.9000 - val_loss: 0.2951\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8617 - loss: 0.3694 - val_accuracy: 0.9000 - val_loss: 0.2845\n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8781 - loss: 0.3531 - val_accuracy: 0.9333 - val_loss: 0.2745\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8835 - loss: 0.3511 - val_accuracy: 0.9667 - val_loss: 0.2633\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9078 - loss: 0.3191 - val_accuracy: 0.9667 - val_loss: 0.2525\n",
      "Epoch 27/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8297 - loss: 0.3736 - val_accuracy: 0.9667 - val_loss: 0.2433\n",
      "Epoch 28/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8872 - loss: 0.3182 - val_accuracy: 0.9667 - val_loss: 0.2338\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8651 - loss: 0.3459 - val_accuracy: 0.9667 - val_loss: 0.2250\n",
      "Epoch 30/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9173 - loss: 0.2643 - val_accuracy: 0.9667 - val_loss: 0.2160\n",
      "Epoch 31/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8841 - loss: 0.3226 - val_accuracy: 0.9667 - val_loss: 0.2080\n",
      "Epoch 32/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9181 - loss: 0.2777 - val_accuracy: 0.9667 - val_loss: 0.2001\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9310 - loss: 0.2524 - val_accuracy: 0.9667 - val_loss: 0.1925\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9045 - loss: 0.2769 - val_accuracy: 0.9667 - val_loss: 0.1854\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9242 - loss: 0.2489 - val_accuracy: 0.9667 - val_loss: 0.1777\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9104 - loss: 0.2393 - val_accuracy: 0.9667 - val_loss: 0.1717\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9158 - loss: 0.2255 - val_accuracy: 0.9667 - val_loss: 0.1654\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9176 - loss: 0.2292 - val_accuracy: 0.9667 - val_loss: 0.1597\n",
      "Epoch 39/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9351 - loss: 0.2125 - val_accuracy: 0.9667 - val_loss: 0.1542\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9442 - loss: 0.2164 - val_accuracy: 0.9667 - val_loss: 0.1487\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9249 - loss: 0.2420 - val_accuracy: 0.9667 - val_loss: 0.1437\n",
      "Epoch 42/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9465 - loss: 0.2089 - val_accuracy: 0.9667 - val_loss: 0.1389\n",
      "Epoch 43/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9314 - loss: 0.2118 - val_accuracy: 0.9667 - val_loss: 0.1341\n",
      "Epoch 44/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9573 - loss: 0.1974 - val_accuracy: 0.9667 - val_loss: 0.1284\n",
      "Epoch 45/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9666 - loss: 0.1866 - val_accuracy: 1.0000 - val_loss: 0.1236\n",
      "Epoch 46/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9461 - loss: 0.1823 - val_accuracy: 1.0000 - val_loss: 0.1191\n",
      "Epoch 47/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9578 - loss: 0.1687 - val_accuracy: 1.0000 - val_loss: 0.1148\n",
      "Epoch 48/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9524 - loss: 0.1642 - val_accuracy: 1.0000 - val_loss: 0.1101\n",
      "Epoch 49/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9636 - loss: 0.1650 - val_accuracy: 1.0000 - val_loss: 0.1064\n",
      "Epoch 50/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9692 - loss: 0.1442 - val_accuracy: 1.0000 - val_loss: 0.1027\n",
      "Epoch 51/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9625 - loss: 0.1546 - val_accuracy: 1.0000 - val_loss: 0.0998\n",
      "Epoch 52/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9630 - loss: 0.1445 - val_accuracy: 1.0000 - val_loss: 0.0972\n",
      "Epoch 53/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9753 - loss: 0.1444 - val_accuracy: 1.0000 - val_loss: 0.0939\n",
      "Epoch 54/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9721 - loss: 0.1242 - val_accuracy: 1.0000 - val_loss: 0.0913\n",
      "Epoch 55/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9859 - loss: 0.1344 - val_accuracy: 1.0000 - val_loss: 0.0890\n",
      "Epoch 56/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9420 - loss: 0.1720 - val_accuracy: 1.0000 - val_loss: 0.0873\n",
      "Epoch 57/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9553 - loss: 0.1430 - val_accuracy: 1.0000 - val_loss: 0.0852\n",
      "Epoch 58/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9806 - loss: 0.1176 - val_accuracy: 1.0000 - val_loss: 0.0827\n",
      "Epoch 59/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9704 - loss: 0.1062 - val_accuracy: 1.0000 - val_loss: 0.0805\n",
      "Epoch 60/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9559 - loss: 0.1411 - val_accuracy: 1.0000 - val_loss: 0.0784\n",
      "Epoch 61/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9556 - loss: 0.1307 - val_accuracy: 1.0000 - val_loss: 0.0766\n",
      "Epoch 62/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9683 - loss: 0.1223 - val_accuracy: 1.0000 - val_loss: 0.0751\n",
      "Epoch 63/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9736 - loss: 0.1059 - val_accuracy: 1.0000 - val_loss: 0.0742\n",
      "Epoch 64/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9837 - loss: 0.0993 - val_accuracy: 1.0000 - val_loss: 0.0723\n",
      "Epoch 65/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9644 - loss: 0.1185 - val_accuracy: 1.0000 - val_loss: 0.0709\n",
      "Epoch 66/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9860 - loss: 0.0940 - val_accuracy: 1.0000 - val_loss: 0.0689\n",
      "Epoch 67/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9870 - loss: 0.1019 - val_accuracy: 1.0000 - val_loss: 0.0684\n",
      "Epoch 68/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9729 - loss: 0.1029 - val_accuracy: 1.0000 - val_loss: 0.0661\n",
      "Epoch 69/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9742 - loss: 0.1021 - val_accuracy: 1.0000 - val_loss: 0.0647\n",
      "Epoch 70/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9723 - loss: 0.0886 - val_accuracy: 1.0000 - val_loss: 0.0635\n",
      "Epoch 71/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9707 - loss: 0.1047 - val_accuracy: 1.0000 - val_loss: 0.0621\n",
      "Epoch 72/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9748 - loss: 0.0938 - val_accuracy: 1.0000 - val_loss: 0.0604\n",
      "Epoch 73/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9800 - loss: 0.0822 - val_accuracy: 1.0000 - val_loss: 0.0598\n",
      "Epoch 74/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9834 - loss: 0.0980 - val_accuracy: 1.0000 - val_loss: 0.0593\n",
      "Epoch 75/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9783 - loss: 0.1051 - val_accuracy: 1.0000 - val_loss: 0.0578\n",
      "Epoch 76/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9928 - loss: 0.0718 - val_accuracy: 1.0000 - val_loss: 0.0566\n",
      "Epoch 77/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9941 - loss: 0.0747 - val_accuracy: 1.0000 - val_loss: 0.0552\n",
      "Epoch 78/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9653 - loss: 0.0847 - val_accuracy: 1.0000 - val_loss: 0.0537\n",
      "Epoch 79/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9846 - loss: 0.0828 - val_accuracy: 1.0000 - val_loss: 0.0525\n",
      "Epoch 80/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9644 - loss: 0.0933 - val_accuracy: 1.0000 - val_loss: 0.0517\n",
      "Epoch 81/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9763 - loss: 0.0799 - val_accuracy: 1.0000 - val_loss: 0.0505\n",
      "Epoch 82/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9811 - loss: 0.0826 - val_accuracy: 1.0000 - val_loss: 0.0500\n",
      "Epoch 83/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9928 - loss: 0.0600 - val_accuracy: 1.0000 - val_loss: 0.0493\n",
      "Epoch 84/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9834 - loss: 0.0733 - val_accuracy: 1.0000 - val_loss: 0.0476\n",
      "Epoch 85/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9632 - loss: 0.0824 - val_accuracy: 1.0000 - val_loss: 0.0470\n",
      "Epoch 86/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9742 - loss: 0.0847 - val_accuracy: 1.0000 - val_loss: 0.0460\n",
      "Epoch 87/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9702 - loss: 0.0849 - val_accuracy: 1.0000 - val_loss: 0.0449\n",
      "Epoch 88/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9816 - loss: 0.0594 - val_accuracy: 1.0000 - val_loss: 0.0442\n",
      "Epoch 89/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9659 - loss: 0.0802 - val_accuracy: 1.0000 - val_loss: 0.0437\n",
      "Epoch 90/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9794 - loss: 0.0674 - val_accuracy: 1.0000 - val_loss: 0.0436\n",
      "Epoch 91/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9694 - loss: 0.0797 - val_accuracy: 1.0000 - val_loss: 0.0437\n",
      "Epoch 92/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9833 - loss: 0.0627 - val_accuracy: 1.0000 - val_loss: 0.0429\n",
      "Epoch 93/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9601 - loss: 0.0896 - val_accuracy: 1.0000 - val_loss: 0.0419\n",
      "Epoch 94/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9866 - loss: 0.0527 - val_accuracy: 1.0000 - val_loss: 0.0412\n",
      "Epoch 95/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9800 - loss: 0.0632 - val_accuracy: 1.0000 - val_loss: 0.0404\n",
      "Epoch 96/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9889 - loss: 0.0572 - val_accuracy: 1.0000 - val_loss: 0.0399\n",
      "Epoch 97/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9906 - loss: 0.0575 - val_accuracy: 1.0000 - val_loss: 0.0405\n",
      "Epoch 98/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9852 - loss: 0.0608 - val_accuracy: 1.0000 - val_loss: 0.0388\n",
      "Epoch 99/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9781 - loss: 0.0558 - val_accuracy: 1.0000 - val_loss: 0.0391\n",
      "Epoch 100/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9804 - loss: 0.0619 - val_accuracy: 1.0000 - val_loss: 0.0399\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.0399\n",
      "📊 Final Loss: 0.0399, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# ✅ 1. Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # 4 features\n",
    "y = iris.target  # 3 classes (0, 1, 2)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# ✅ One-Hot Encoding \n",
    "encoder = OneHotEncoder(sparse_output=False)  # Required for Keras since 'categorical_crossentropy' does not handle class indices directly\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))  # Convert labels to one-hot encoded format\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ 2. Define the MLP model in Keras\n",
    "keras_model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation='relu', input_shape=(4,)),  # 4 inputs → 16 neurons\n",
    "    keras.layers.Dense(8, activation='relu'),  # 8 hidden neurons\n",
    "    keras.layers.Dense(3, activation='softmax')  # 3 output classes → softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# ✅ 3. Compile the model\n",
    "keras_model.compile(optimizer='adam', \n",
    "                    loss='categorical_crossentropy',  # CrossEntropy for multi-class classification\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# ✅ 4. Train the model\n",
    "keras_model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# ✅ 5. Evaluate the model\n",
    "loss, accuracy = keras_model.evaluate(X_test, y_test)\n",
    "print(f\"📊 Final Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 Advanced Topics in Neural Networks - Dropout, Batch Normalization & More\n",
    "\n",
    "Now that we've covered the basics of Neural Networks, let's dive into **advanced techniques** to improve performance:\n",
    "✔ **Dropout: Regularization to prevent overfitting**  \n",
    "✔ **Batch Normalization: Stabilizing training & speeding up convergence**  \n",
    "✔ **Weight Initialization: Preventing vanishing/exploding gradients**  \n",
    "✔ **Gradient Clipping: Handling exploding gradients**  \n",
    "✔ **Learning Rate Scheduling: Adaptive learning rates**  \n",
    "\n",
    "---\n",
    "\n",
    "# 📖 8. Dropout - Preventing Overfitting\n",
    "\n",
    "### 🔹 1. What is Dropout?\n",
    "\n",
    "**Dropout** is a **regularization technique** used to **prevent overfitting** by randomly dropping units (neurons) during training.\n",
    "\n",
    "### 🔹 2. How Dropout Works\n",
    "1. At each training step, randomly **drop neurons** with probability **$p$**.\n",
    "2. Forward pass continues **without these neurons**.\n",
    "3. At test time, **all neurons are active** but their outputs are scaled by **$1 - p$** to maintain consistency.\n",
    "\n",
    "### 🔹 3. Mathematical Formulation\n",
    "If **$h_i$** is the output of neuron **$i$**, then with dropout:\n",
    "\n",
    "$$\n",
    "h_i^{\\text{drop}} = \\frac{h_i}{1 - p} \\quad \\text{(at test time)}\n",
    "$$\n",
    "\n",
    "where **$p$** is the dropout probability.\n",
    "\n",
    "✅ **Why use Dropout?**\n",
    "- Reduces **co-adaptation** between neurons.\n",
    "- Acts as **ensemble learning** (averaging different sub-networks).\n",
    "- Helps **generalization** by forcing the model to be more robust.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Implementing Dropout in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10/100 | Loss : 0.0037\n",
      "Epoch : 20/100 | Loss : 0.2505\n",
      "Epoch : 30/100 | Loss : 0.0208\n",
      "Epoch : 40/100 | Loss : 0.2093\n",
      "Epoch : 50/100 | Loss : 0.2274\n",
      "Epoch : 60/100 | Loss : 0.0105\n",
      "Epoch : 70/100 | Loss : 0.0128\n",
      "Epoch : 80/100 | Loss : 0.0000\n",
      "Epoch : 90/100 | Loss : 0.0000\n",
      "Epoch : 100/100 | Loss : 0.0062\n",
      "Accuracy : 46.666666666666664\n",
      "We observed that the Dropout Rate=0.5 is too much for this type of task\n"
     ]
    }
   ],
   "source": [
    "# Define MLP model with Dropout\n",
    "class MLP_Dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # 50% neurons dropped\n",
    "        self.layer2 = nn.Linear(4, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.activation(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model_dropout = MLP_Dropout()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch : {epoch+1}/{epochs} | Loss : {loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct = (predicted==y_batch).sum().item()\n",
    "accuracy = 100 * correct/total\n",
    "\n",
    "\n",
    "print(f\"Accuracy : {accuracy}\")\n",
    "print(\"We observed that the Dropout Rate=0.5 is too much for this type of task\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Implementing Dropout in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Define MLP model with Dropout\n",
    "model_dropout = keras.Sequential([\n",
    "    keras.layers.Dense(4, activation='relu', input_shape=(2,)),\n",
    "    keras.layers.Dropout(0.5),  # Dropout layer with 50% probability\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_dropout.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to train and test this model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Implementing Batch Normalization in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model with BatchNorm\n",
    "class MLP_BatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        self.batchnorm = nn.BatchNorm1d(4)  # Batch Normalization layer\n",
    "        self.layer2 = nn.Linear(4, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.batchnorm(x)  # Apply batch normalization\n",
    "        x = torch.relu(x)\n",
    "        x = self.activation(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model_batchnorm = MLP_BatchNorm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to train and test this model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Implementing Batch Normalization in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Define MLP model with Batch Normalization\n",
    "model_batchnorm = keras.Sequential([\n",
    "    keras.layers.Dense(4, input_shape=(2,), activation='relu'),\n",
    "    keras.layers.BatchNormalization(),  # Batch Normalization layer\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_batchnorm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to train and test this model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📖 9. Other Advanced Topics in Neural Networks\n",
    "\n",
    "In this section, we explore **advanced techniques** that improve training efficiency and stability:\n",
    "\n",
    "✔ **Weight Initialization:** Preventing vanishing/exploding gradients  \n",
    "✔ **Gradient Clipping:** Handling unstable gradients  \n",
    "✔ **Learning Rate Scheduling:** Adapting learning rates dynamically  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 1. Weight Initialization\n",
    "\n",
    "### 🔹 Why is Weight Initialization Important?\n",
    "- **Prevents vanishing/exploding gradients**  \n",
    "- **Speeds up convergence**  \n",
    "- **Ensures stable flow of activations through the network**  \n",
    "\n",
    "### 🔹 Xavier (Glorot) Initialization\n",
    "Used for **sigmoid & tanh activations**, ensures variance remains stable:\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{N}(0, \\frac{1}{n_{\\text{in}}})\n",
    "$$\n",
    "\n",
    "### 🔹 He Initialization\n",
    "Used for **ReLU & Leaky ReLU activations**, accounts for rectification:\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{N}(0, \\frac{2}{n_{\\text{in}}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Implementing Xavier & He Initialization in PyTorch\n",
    "\n",
    "# Xavier (Glorot) Initialization\n",
    "def xavier_init(shape):\n",
    "    return torch.randn(shape) * torch.sqrt(torch.tensor(1.0) / shape[1])\n",
    "\n",
    "# He Initialization (for ReLU networks)\n",
    "def he_init(shape):\n",
    "    return torch.randn(shape) * torch.sqrt(torch.tensor(2.0) / shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Implementing Xavier & He Initialization in Keras\n",
    "from tensorflow.keras.initializers import GlorotUniform, HeNormal\n",
    "\n",
    "# Xavier Initialization (Glorot)\n",
    "xavier_initializer = GlorotUniform()\n",
    "\n",
    "# He Initialization (for ReLU)\n",
    "he_initializer = HeNormal()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 2. Gradient Clipping\n",
    "\n",
    "### 🔹 Why Use Gradient Clipping?\n",
    "- **Prevents exploding gradients**, especially in deep networks or RNNs  \n",
    "- **Ensures stability** in weight updates  \n",
    "\n",
    "### 🔹 Gradient Clipping Formula\n",
    "\n",
    "For each weight **$w_i$**, if its gradient **$\\nabla w_i$** exceeds a threshold **$c$**, rescale:\n",
    "\n",
    "$$\n",
    "\\nabla w_i = \\frac{c}{||\\nabla w||} \\nabla w_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.0010\n",
      "Epoch 20/100, Loss: 0.0054\n",
      "Epoch 30/100, Loss: 0.0043\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0003\n",
      "Epoch 60/100, Loss: 0.0002\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0070\n",
      "Epoch 90/100, Loss: 0.0059\n",
      "Epoch 100/100, Loss: 0.0109\n"
     ]
    }
   ],
   "source": [
    "# ✅ Training the model with Gradient Clipping\n",
    "epochs = 100\n",
    "clip_value = 1.0  # Define the maximum gradient norm\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # 🚀 Apply Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_value)\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5756 - loss: 0.8960 - val_accuracy: 0.8667 - val_loss: 0.5415\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8330 - loss: 0.5218 - val_accuracy: 0.9000 - val_loss: 0.3303\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7957 - loss: 0.4488 - val_accuracy: 0.9667 - val_loss: 0.2327\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9155 - loss: 0.2742 - val_accuracy: 0.9333 - val_loss: 0.1781\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9316 - loss: 0.2395 - val_accuracy: 0.9667 - val_loss: 0.1565\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9390 - loss: 0.2148 - val_accuracy: 1.0000 - val_loss: 0.1262\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9357 - loss: 0.1949 - val_accuracy: 1.0000 - val_loss: 0.1119\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9560 - loss: 0.1581 - val_accuracy: 0.9667 - val_loss: 0.0943\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9558 - loss: 0.1086 - val_accuracy: 1.0000 - val_loss: 0.0931\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9669 - loss: 0.1043 - val_accuracy: 0.9667 - val_loss: 0.0831\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9487 - loss: 0.1061 - val_accuracy: 1.0000 - val_loss: 0.0682\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9589 - loss: 0.0895 - val_accuracy: 1.0000 - val_loss: 0.0522\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9752 - loss: 0.0673 - val_accuracy: 0.9667 - val_loss: 0.0688\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9672 - loss: 0.0721 - val_accuracy: 1.0000 - val_loss: 0.0635\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9691 - loss: 0.0867 - val_accuracy: 1.0000 - val_loss: 0.0619\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9918 - loss: 0.0523 - val_accuracy: 0.9667 - val_loss: 0.0574\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9562 - loss: 0.0779 - val_accuracy: 1.0000 - val_loss: 0.0431\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9852 - loss: 0.0636 - val_accuracy: 0.9667 - val_loss: 0.0629\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9662 - loss: 0.0617 - val_accuracy: 1.0000 - val_loss: 0.0398\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9880 - loss: 0.0486 - val_accuracy: 1.0000 - val_loss: 0.0387\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9849 - loss: 0.0481 - val_accuracy: 0.9667 - val_loss: 0.0441\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9708 - loss: 0.0565 - val_accuracy: 1.0000 - val_loss: 0.0513\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9875 - loss: 0.0379 - val_accuracy: 1.0000 - val_loss: 0.0359\n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9887 - loss: 0.0455 - val_accuracy: 1.0000 - val_loss: 0.0337\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9860 - loss: 0.0326 - val_accuracy: 1.0000 - val_loss: 0.0388\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9906 - loss: 0.0319 - val_accuracy: 1.0000 - val_loss: 0.0311\n",
      "Epoch 27/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9875 - loss: 0.0370 - val_accuracy: 1.0000 - val_loss: 0.0275\n",
      "Epoch 28/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9847 - loss: 0.0308 - val_accuracy: 1.0000 - val_loss: 0.0297\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9857 - loss: 0.0352 - val_accuracy: 1.0000 - val_loss: 0.0281\n",
      "Epoch 30/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9889 - loss: 0.0514 - val_accuracy: 1.0000 - val_loss: 0.0227\n",
      "Epoch 31/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9748 - loss: 0.0494 - val_accuracy: 1.0000 - val_loss: 0.0191\n",
      "Epoch 32/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9918 - loss: 0.0247 - val_accuracy: 1.0000 - val_loss: 0.0290\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9748 - loss: 0.0520 - val_accuracy: 0.9667 - val_loss: 0.0568\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9776 - loss: 0.0502 - val_accuracy: 1.0000 - val_loss: 0.0473\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9852 - loss: 0.0375 - val_accuracy: 1.0000 - val_loss: 0.0211\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9866 - loss: 0.0405 - val_accuracy: 1.0000 - val_loss: 0.0206\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9733 - loss: 0.0490 - val_accuracy: 1.0000 - val_loss: 0.0234\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9650 - loss: 0.0651 - val_accuracy: 0.9667 - val_loss: 0.0364\n",
      "Epoch 39/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9875 - loss: 0.0354 - val_accuracy: 1.0000 - val_loss: 0.0360\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9875 - loss: 0.0297 - val_accuracy: 1.0000 - val_loss: 0.0257\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9811 - loss: 0.0398 - val_accuracy: 1.0000 - val_loss: 0.0272\n",
      "Epoch 42/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9742 - loss: 0.0425 - val_accuracy: 1.0000 - val_loss: 0.0268\n",
      "Epoch 43/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9730 - loss: 0.0392 - val_accuracy: 1.0000 - val_loss: 0.0298\n",
      "Epoch 44/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9875 - loss: 0.0314 - val_accuracy: 1.0000 - val_loss: 0.0245\n",
      "Epoch 45/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9816 - loss: 0.0272 - val_accuracy: 1.0000 - val_loss: 0.0221\n",
      "Epoch 46/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9776 - loss: 0.0469 - val_accuracy: 1.0000 - val_loss: 0.0389\n",
      "Epoch 47/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9852 - loss: 0.0305 - val_accuracy: 0.9667 - val_loss: 0.0455\n",
      "Epoch 48/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9574 - loss: 0.0838 - val_accuracy: 1.0000 - val_loss: 0.0188\n",
      "Epoch 49/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9800 - loss: 0.0553 - val_accuracy: 1.0000 - val_loss: 0.0284\n",
      "Epoch 50/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9730 - loss: 0.0448 - val_accuracy: 1.0000 - val_loss: 0.0280\n",
      "Epoch 51/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9929 - loss: 0.0238 - val_accuracy: 1.0000 - val_loss: 0.0262\n",
      "Epoch 52/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9824 - loss: 0.0334 - val_accuracy: 1.0000 - val_loss: 0.0311\n",
      "Epoch 53/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9742 - loss: 0.0595 - val_accuracy: 1.0000 - val_loss: 0.0335\n",
      "Epoch 54/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9814 - loss: 0.0458 - val_accuracy: 1.0000 - val_loss: 0.0228\n",
      "Epoch 55/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9871 - loss: 0.0511 - val_accuracy: 1.0000 - val_loss: 0.0473\n",
      "Epoch 56/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9932 - loss: 0.0300 - val_accuracy: 1.0000 - val_loss: 0.0253\n",
      "Epoch 57/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9864 - loss: 0.0433 - val_accuracy: 1.0000 - val_loss: 0.0300\n",
      "Epoch 58/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9929 - loss: 0.0326 - val_accuracy: 0.9667 - val_loss: 0.0609\n",
      "Epoch 59/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9831 - loss: 0.0375 - val_accuracy: 0.9667 - val_loss: 0.0424\n",
      "Epoch 60/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9892 - loss: 0.0473 - val_accuracy: 0.9667 - val_loss: 0.0349\n",
      "Epoch 61/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9906 - loss: 0.0171 - val_accuracy: 1.0000 - val_loss: 0.0312\n",
      "Epoch 62/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9918 - loss: 0.0238 - val_accuracy: 1.0000 - val_loss: 0.0279\n",
      "Epoch 63/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9866 - loss: 0.0242 - val_accuracy: 1.0000 - val_loss: 0.0265\n",
      "Epoch 64/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9831 - loss: 0.0295 - val_accuracy: 1.0000 - val_loss: 0.0247\n",
      "Epoch 65/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9877 - loss: 0.0273 - val_accuracy: 1.0000 - val_loss: 0.0303\n",
      "Epoch 66/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9972 - loss: 0.0184 - val_accuracy: 1.0000 - val_loss: 0.0294\n",
      "Epoch 67/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9946 - loss: 0.0202 - val_accuracy: 1.0000 - val_loss: 0.0246\n",
      "Epoch 68/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9953 - loss: 0.0188 - val_accuracy: 1.0000 - val_loss: 0.0281\n",
      "Epoch 69/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9776 - loss: 0.0359 - val_accuracy: 0.9667 - val_loss: 0.0369\n",
      "Epoch 70/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9900 - loss: 0.0257 - val_accuracy: 1.0000 - val_loss: 0.0379\n",
      "Epoch 71/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9877 - loss: 0.0220 - val_accuracy: 1.0000 - val_loss: 0.0358\n",
      "Epoch 72/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9783 - loss: 0.0276 - val_accuracy: 1.0000 - val_loss: 0.0294\n",
      "Epoch 73/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9900 - loss: 0.0234 - val_accuracy: 1.0000 - val_loss: 0.0419\n",
      "Epoch 74/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9928 - loss: 0.0187 - val_accuracy: 1.0000 - val_loss: 0.0322\n",
      "Epoch 75/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9837 - loss: 0.0356 - val_accuracy: 0.9667 - val_loss: 0.0624\n",
      "Epoch 76/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9817 - loss: 0.0323 - val_accuracy: 0.9667 - val_loss: 0.0852\n",
      "Epoch 77/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9953 - loss: 0.0120 - val_accuracy: 1.0000 - val_loss: 0.0394\n",
      "Epoch 78/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9882 - loss: 0.0255 - val_accuracy: 0.9667 - val_loss: 0.0597\n",
      "Epoch 79/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9972 - loss: 0.0116 - val_accuracy: 0.9667 - val_loss: 0.0592\n",
      "Epoch 80/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9783 - loss: 0.0403 - val_accuracy: 0.9667 - val_loss: 0.0475\n",
      "Epoch 81/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9960 - loss: 0.0222 - val_accuracy: 1.0000 - val_loss: 0.0409\n",
      "Epoch 82/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9866 - loss: 0.0197 - val_accuracy: 0.9667 - val_loss: 0.0443\n",
      "Epoch 83/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9932 - loss: 0.0113 - val_accuracy: 1.0000 - val_loss: 0.0285\n",
      "Epoch 84/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9929 - loss: 0.0214 - val_accuracy: 1.0000 - val_loss: 0.0335\n",
      "Epoch 85/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9817 - loss: 0.0211 - val_accuracy: 1.0000 - val_loss: 0.0326\n",
      "Epoch 86/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9742 - loss: 0.0279 - val_accuracy: 1.0000 - val_loss: 0.0322\n",
      "Epoch 87/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9960 - loss: 0.0215 - val_accuracy: 0.9667 - val_loss: 0.0805\n",
      "Epoch 88/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9906 - loss: 0.0228 - val_accuracy: 1.0000 - val_loss: 0.0524\n",
      "Epoch 89/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9887 - loss: 0.0242 - val_accuracy: 0.9667 - val_loss: 0.0466\n",
      "Epoch 90/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9981 - loss: 0.0147 - val_accuracy: 1.0000 - val_loss: 0.0491\n",
      "Epoch 91/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9929 - loss: 0.0185 - val_accuracy: 0.9667 - val_loss: 0.0889\n",
      "Epoch 92/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9846 - loss: 0.0222 - val_accuracy: 0.9667 - val_loss: 0.0537\n",
      "Epoch 93/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9817 - loss: 0.0420 - val_accuracy: 0.9667 - val_loss: 0.0727\n",
      "Epoch 94/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9889 - loss: 0.0187 - val_accuracy: 0.9667 - val_loss: 0.0503\n",
      "Epoch 95/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9773 - loss: 0.0329 - val_accuracy: 1.0000 - val_loss: 0.0351\n",
      "Epoch 96/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9946 - loss: 0.0205 - val_accuracy: 0.9667 - val_loss: 0.0618\n",
      "Epoch 97/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9929 - loss: 0.0159 - val_accuracy: 0.9667 - val_loss: 0.0761\n",
      "Epoch 98/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9875 - loss: 0.0156 - val_accuracy: 0.9333 - val_loss: 0.0818\n",
      "Epoch 99/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9906 - loss: 0.0208 - val_accuracy: 0.9667 - val_loss: 0.0575\n",
      "Epoch 100/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9906 - loss: 0.0155 - val_accuracy: 0.9667 - val_loss: 0.0476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x177592f8200>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ Define the MLP model in Keras with Gradient Clipping\n",
    "keras_model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation='relu', input_shape=(4,)),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# ✅ Define the optimizer with Gradient Clipping\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01, clipnorm=1.0)  # Clip gradient norm\n",
    "\n",
    "# ✅ Compile the model\n",
    "keras_model.compile(optimizer=optimizer, \n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# ✅ Train the model\n",
    "keras_model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=1, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 3. Learning Rate Scheduling\n",
    "\n",
    "### 🔹 Why Use Learning Rate Scheduling?\n",
    "- **Adjusts learning rate dynamically** to **improve convergence**  \n",
    "- **Speeds up training** while avoiding overshooting  \n",
    "\n",
    "### 🔹 Common Learning Rate Schedulers\n",
    "\n",
    "#### **1. Exponential Decay**\n",
    "Gradually reduces the learning rate:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\alpha_0 \\times e^{-kt}\n",
    "$$\n",
    "\n",
    "#### **2. Step Decay**\n",
    "Reduces the learning rate **after a fixed number of steps**:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\alpha_0 \\times 0.5^{t//k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.005000\n",
      "Epoch 20, Train Loss: 0.0057, Val Loss: 0.0009, LR: 0.002500\n",
      "Epoch 30, Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.000625\n",
      "Epoch 40, Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.000156\n",
      "Epoch 50, Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.000078\n",
      "Epoch 60, Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.000020\n",
      "Epoch 70, Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.000005\n",
      "Epoch 80, Train Loss: 0.0005, Val Loss: 0.0009, LR: 0.000002\n",
      "Epoch 90, Train Loss: 0.0004, Val Loss: 0.0009, LR: 0.000001\n",
      "Epoch 100, Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.000000\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ✅ Learning rate scheduler (Reduce LR on Plateau)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# ✅ Training loop with LR scheduler\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ✅ Compute validation loss before updating LR\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss += criterion(y_val_pred, y_val).item()\n",
    "\n",
    "    val_loss /= len(test_loader)  # ✅ Compute average validation loss\n",
    "\n",
    "    # ✅ Adjust learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # ✅ Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9801 - loss: 0.0313 - val_accuracy: 0.9667 - val_loss: 0.0692 - learning_rate: 0.0100\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9946 - loss: 0.0226 - val_accuracy: 0.9667 - val_loss: 0.0778 - learning_rate: 0.0100\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9871 - loss: 0.0197 - val_accuracy: 0.9667 - val_loss: 0.0915 - learning_rate: 0.0100\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9981 - loss: 0.0082 - val_accuracy: 0.9333 - val_loss: 0.0836 - learning_rate: 0.0100\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9981 - loss: 0.0107 - val_accuracy: 0.9667 - val_loss: 0.0753 - learning_rate: 0.0100\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9929 - loss: 0.0168 - val_accuracy: 0.9667 - val_loss: 0.1013 - learning_rate: 0.0100\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9801 - loss: 0.0371 - val_accuracy: 0.9333 - val_loss: 0.0682 - learning_rate: 0.0050\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0197 - val_accuracy: 0.9333 - val_loss: 0.0729 - learning_rate: 0.0050\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0195 - val_accuracy: 0.9667 - val_loss: 0.0748 - learning_rate: 0.0050\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9871 - loss: 0.0235 - val_accuracy: 0.9333 - val_loss: 0.0796 - learning_rate: 0.0050\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9972 - loss: 0.0129 - val_accuracy: 0.9333 - val_loss: 0.0728 - learning_rate: 0.0050\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9981 - loss: 0.0093 - val_accuracy: 0.9667 - val_loss: 0.0701 - learning_rate: 0.0050\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0077 - val_accuracy: 0.9667 - val_loss: 0.0617 - learning_rate: 0.0025\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0151 - val_accuracy: 0.9667 - val_loss: 0.0617 - learning_rate: 0.0025\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0113 - val_accuracy: 0.9667 - val_loss: 0.0686 - learning_rate: 0.0025\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0122 - val_accuracy: 0.9667 - val_loss: 0.0697 - learning_rate: 0.0025\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0124 - val_accuracy: 0.9333 - val_loss: 0.0687 - learning_rate: 0.0025\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0165 - val_accuracy: 0.9667 - val_loss: 0.0705 - learning_rate: 0.0025\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0141 - val_accuracy: 0.9667 - val_loss: 0.0707 - learning_rate: 0.0012\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0136 - val_accuracy: 0.9667 - val_loss: 0.0694 - learning_rate: 0.0012\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0063 - val_accuracy: 0.9667 - val_loss: 0.0691 - learning_rate: 0.0012\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0096 - val_accuracy: 0.9667 - val_loss: 0.0666 - learning_rate: 0.0012\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0100 - val_accuracy: 0.9667 - val_loss: 0.0671 - learning_rate: 0.0012\n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0107 - val_accuracy: 0.9667 - val_loss: 0.0637 - learning_rate: 6.2500e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0254 - val_accuracy: 0.9667 - val_loss: 0.0635 - learning_rate: 6.2500e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0096 - val_accuracy: 0.9667 - val_loss: 0.0649 - learning_rate: 6.2500e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0111 - val_accuracy: 0.9667 - val_loss: 0.0666 - learning_rate: 6.2500e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0153 - val_accuracy: 0.9667 - val_loss: 0.0670 - learning_rate: 6.2500e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0110 - val_accuracy: 0.9667 - val_loss: 0.0668 - learning_rate: 3.1250e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0140 - val_accuracy: 0.9667 - val_loss: 0.0664 - learning_rate: 3.1250e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0207 - val_accuracy: 0.9667 - val_loss: 0.0662 - learning_rate: 3.1250e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0081 - val_accuracy: 0.9667 - val_loss: 0.0660 - learning_rate: 3.1250e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0140 - val_accuracy: 0.9667 - val_loss: 0.0661 - learning_rate: 3.1250e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0128 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.5625e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0123 - val_accuracy: 0.9667 - val_loss: 0.0662 - learning_rate: 1.5625e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0097 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.5625e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0164 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.5625e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 0.9667 - val_loss: 0.0667 - learning_rate: 1.5625e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0180 - val_accuracy: 0.9667 - val_loss: 0.0664 - learning_rate: 7.8125e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0182 - val_accuracy: 0.9667 - val_loss: 0.0664 - learning_rate: 7.8125e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0103 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 7.8125e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0179 - val_accuracy: 0.9667 - val_loss: 0.0666 - learning_rate: 7.8125e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0097 - val_accuracy: 0.9667 - val_loss: 0.0666 - learning_rate: 7.8125e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0097 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.9062e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0188 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.9062e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0156 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.9062e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 0.9667 - val_loss: 0.0666 - learning_rate: 3.9062e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0145 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.9062e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0073 - val_accuracy: 0.9667 - val_loss: 0.0666 - learning_rate: 1.9531e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0167 - val_accuracy: 0.9667 - val_loss: 0.0666 - learning_rate: 1.9531e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0154 - val_accuracy: 0.9667 - val_loss: 0.0666 - learning_rate: 1.9531e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0170 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.9531e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0095 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.9531e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0175 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 9.7656e-06\n",
      "Epoch 55/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0116 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 9.7656e-06\n",
      "Epoch 56/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0101 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 9.7656e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0111 - val_accuracy: 0.9667 - val_loss: 0.0666 - learning_rate: 9.7656e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0115 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 9.7656e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0154 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 4.8828e-06\n",
      "Epoch 60/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0137 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 4.8828e-06\n",
      "Epoch 61/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 4.8828e-06\n",
      "Epoch 62/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0133 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 4.8828e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0072 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 4.8828e-06\n",
      "Epoch 64/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0210 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 2.4414e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0133 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 2.4414e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0125 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 2.4414e-06\n",
      "Epoch 67/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0090 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 2.4414e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0186 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 2.4414e-06\n",
      "Epoch 69/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0191 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.2207e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0195 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.2207e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0161 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.2207e-06\n",
      "Epoch 72/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0113 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.2207e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.2207e-06\n",
      "Epoch 74/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0110 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 6.1035e-07\n",
      "Epoch 75/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0125 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 6.1035e-07\n",
      "Epoch 76/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0139 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 6.1035e-07\n",
      "Epoch 77/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0164 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 6.1035e-07\n",
      "Epoch 78/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 6.1035e-07\n",
      "Epoch 79/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0124 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.0518e-07\n",
      "Epoch 80/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.0518e-07\n",
      "Epoch 81/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0097 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.0518e-07\n",
      "Epoch 82/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0127 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.0518e-07\n",
      "Epoch 83/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0097 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.0518e-07\n",
      "Epoch 84/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0176 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.5259e-07\n",
      "Epoch 85/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0095 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.5259e-07\n",
      "Epoch 86/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.5259e-07\n",
      "Epoch 87/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0168 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.5259e-07\n",
      "Epoch 88/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.5259e-07\n",
      "Epoch 89/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 7.6294e-08\n",
      "Epoch 90/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0129 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 7.6294e-08\n",
      "Epoch 91/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 7.6294e-08\n",
      "Epoch 92/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0105 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 7.6294e-08\n",
      "Epoch 93/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0138 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 7.6294e-08\n",
      "Epoch 94/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.8147e-08\n",
      "Epoch 95/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0137 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.8147e-08\n",
      "Epoch 96/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0208 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.8147e-08\n",
      "Epoch 97/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0122 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.8147e-08\n",
      "Epoch 98/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0068 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 3.8147e-08\n",
      "Epoch 99/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0110 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.9073e-08\n",
      "Epoch 100/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0168 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 1.9073e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x177592f9910>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ Learning rate scheduler (Reduce LR on Plateau)\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# ✅ Train the model with the scheduler\n",
    "keras_model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=1, validation_data=(X_test, y_test), callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 10. Interview Questions\n",
    "\n",
    "### 🔹 **Dropout Questions**\n",
    "1️⃣ **Why does Dropout help prevent overfitting?**  \n",
    "   - Forces the network to learn redundant representations.  \n",
    "   \n",
    "2️⃣ **How does Dropout affect training vs. inference?**  \n",
    "   - During training, neurons are randomly dropped with probability **p**.  \n",
    "   - During inference, all neurons are active, but their outputs are scaled by **(1 - p)** to maintain consistency.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Batch Normalization Questions**\n",
    "3️⃣ **Why does BatchNorm improve training?**  \n",
    "   - It stabilizes learning, reduces internal covariate shift, and allows for higher learning rates.  \n",
    "\n",
    "4️⃣ **How does BatchNorm act as a regularization technique?**  \n",
    "   - It introduces slight noise to the activations, similar to Dropout, which reduces overfitting.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Optimization Questions**\n",
    "5️⃣ **Why use Xavier (Glorot) initialization?**  \n",
    "   - It ensures that the variance of activations remains stable across layers, preventing vanishing/exploding gradients.  \n",
    "\n",
    "6️⃣ **Why use He initialization for ReLU networks?**  \n",
    "   - He initialization scales weights to maintain variance in ReLU-based networks, improving convergence.  \n",
    "\n",
    "7️⃣ **What is the purpose of Gradient Clipping?**  \n",
    "   - It prevents exploding gradients by capping their magnitude during backpropagation.  \n",
    "\n",
    "8️⃣ **When should you use Learning Rate Scheduling?**  \n",
    "   - When training loss plateaus, reducing the learning rate helps the model converge better.  \n",
    "\n",
    "9️⃣ **What are the different types of Learning Rate Schedulers?**  \n",
    "   - **Exponential Decay:** Decreases learning rate exponentially over time.  \n",
    "   - **Step Decay:** Reduces learning rate after fixed steps.  \n",
    "   - **Adaptive Methods (Adam, RMSprop):** Adjusts learning rates dynamically based on past gradients.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Weight Initialization Questions**\n",
    "1️⃣ **Why is weight initialization important?**  \n",
    "   - Prevents vanishing/exploding gradients.  \n",
    "\n",
    "2️⃣ **When should you use Xavier initialization?**  \n",
    "   - For **sigmoid and tanh** activation functions.  \n",
    "\n",
    "3️⃣ **When should you use He initialization?**  \n",
    "   - For **ReLU and Leaky ReLU** activations.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Gradient Clipping Questions**\n",
    "4️⃣ **Why do we clip gradients?**  \n",
    "   - To prevent **exploding gradients**, especially in deep networks and RNNs.  \n",
    "\n",
    "5️⃣ **What is the difference between `clipnorm` and `clipvalue`?**  \n",
    "   - **`clipnorm`** rescales gradients if their **L2 norm exceeds a threshold**.  \n",
    "   - **`clipvalue`** clips each gradient component **individually**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Learning Rate Scheduling Questions**\n",
    "6️⃣ **Why use a dynamic learning rate instead of a fixed one?**  \n",
    "   - **Higher learning rates** speed up training initially.  \n",
    "   - **Lower learning rates** improve convergence at later stages.  \n",
    "\n",
    "7️⃣ **What’s the difference between Exponential Decay and Step Decay?**  \n",
    "   - **Exponential Decay** gradually decreases learning rate every step.  \n",
    "   - **Step Decay** reduces learning rate **at fixed intervals**.  \n",
    "\n",
    "8️⃣ **When should you decay the learning rate?**  \n",
    "   - When the loss **plateaus** and is no longer improving.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
