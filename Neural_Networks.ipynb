{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 Neural Networks (Basics) - A Complete Guide\n",
    "\n",
    "This notebook provides a **detailed breakdown** of fundamental neural network concepts, including:  \n",
    "✔ **Perceptron & Multi-layer Perceptron (MLP)**  \n",
    "✔ **Activation Functions & Their Derivatives**  \n",
    "✔ **Forward & Backpropagation with Detailed Math**  \n",
    "✔ **Gradient Descent for Neural Networks**  \n",
    "✔ **Implementation in PyTorch & Keras**\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 1. Introduction to Neural Networks\n",
    "\n",
    "Neural Networks are inspired by the **human brain** and consist of **layers of neurons** that transform input data through weighted connections.  \n",
    "\n",
    "### 🔹 Why Use Neural Networks?\n",
    "✔ **Can model complex relationships** between inputs and outputs.  \n",
    "✔ **Can approximate any function (Universal Approximation Theorem)**.  \n",
    "✔ **Used in deep learning applications like NLP, Vision, and Reinforcement Learning**.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 2. The Perceptron: The Basic Building Block\n",
    "\n",
    "### 🔹 1. The Perceptron Model  \n",
    "A perceptron is a **single-layer neural network** that performs **binary classification**:\n",
    "\n",
    "$$\n",
    "y = f(W \\cdot X + b)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **$X$** = Input vector\n",
    "- **$W$** = Weights\n",
    "- **$b$** = Bias\n",
    "- **$f$** = Activation function\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Perceptron Learning Rule\n",
    "\n",
    "1. Initialize weights randomly.  \n",
    "2. Compute the **weighted sum** of inputs:  \n",
    "\n",
    "   $$\n",
    "   z = W \\cdot X + b\n",
    "   $$\n",
    "\n",
    "3. Apply an **activation function** (e.g., step function):\n",
    "\n",
    "   $$\n",
    "   y = \\text{sign}(z)\n",
    "   $$\n",
    "\n",
    "4. **Update weights** using the perceptron update rule:\n",
    "\n",
    "   $$\n",
    "   W := W + \\alpha (y_{\\text{true}} - y_{\\text{pred}}) X\n",
    "   $$\n",
    "\n",
    "5. Repeat until convergence.\n",
    "\n",
    "✅ **Limitations**:  \n",
    "- Can only solve **linearly separable** problems (e.g., AND, OR).\n",
    "- **Cannot solve XOR** → Requires Multi-Layer Perceptrons (MLPs).\n",
    "\n",
    "---\n",
    "\n",
    "# 📖 3. Multi-Layer Perceptron (MLP)\n",
    "\n",
    "A **Multi-Layer Perceptron (MLP)** consists of:\n",
    "- **Input Layer**\n",
    "- **Hidden Layers** (with non-linear activations)\n",
    "- **Output Layer**\n",
    "\n",
    "Each neuron in layer **$l$** receives input **$a^{(l-1)}$** from the previous layer:\n",
    "\n",
    "$$\n",
    "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{(l)} = f(z^{(l)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **$W^{(l)}$** = Weights of layer **$l$**  \n",
    "- **$b^{(l)}$** = Bias  \n",
    "- **$f$** = Activation function  \n",
    "\n",
    "✅ **MLPs can learn complex patterns, including XOR**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 4. Activation Functions & Their Derivatives\n",
    "\n",
    "### 🔹 1. Sigmoid Function\n",
    "Used in binary classification:\n",
    "\n",
    "$$\n",
    "f(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Derivative:\n",
    "\n",
    "$$\n",
    "f'(z) = f(z) (1 - f(z))\n",
    "$$\n",
    "\n",
    "✅ **Smooth, differentiable**  \n",
    "⚠ **Suffers from vanishing gradients**  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. ReLU (Rectified Linear Unit)\n",
    "Most popular in deep networks:\n",
    "\n",
    "$$\n",
    "f(z) = \\max(0, z)\n",
    "$$\n",
    "\n",
    "Derivative:\n",
    "\n",
    "$$\n",
    "f'(z) =\n",
    "\\begin{cases}\n",
    "1, & z > 0 \\\\\n",
    "0, & z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "✅ **Faster convergence**  \n",
    "⚠ **Can cause dead neurons (ReLU dying problem)**  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 3. Softmax Function\n",
    "Used in multi-class classification:\n",
    "\n",
    "$$\n",
    "f(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial z_i} = f(z_i) (1 - f(z_i))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 5. Backpropagation: Training Neural Networks\n",
    "\n",
    "To train an MLP, we use **Backpropagation**, which updates weights based on the **chain rule of calculus**.\n",
    "\n",
    "### 🔹 1. Compute Loss Function\n",
    "For **binary classification**, we use **Binary Cross-Entropy**:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "For **multi-class classification**, we use **Categorical Cross-Entropy**:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\sum_{i=1}^{m} \\sum_{j=1}^{k} y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Compute Gradients Using Chain Rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W} = \\frac{\\partial J}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 6. Implementing Neural Networks in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ugo11\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 📦 Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# ✅ Generate synthetic data (XOR problem)\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(y)\n",
    "\n",
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        self.layer2 = nn.Linear(4, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "pt_model = MLP()\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(pt_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train model\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = pt_model(X_tensor)\n",
    "    loss = criterion(output, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate model\n",
    "print(f\"Final loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 7. Implementing Neural Networks in Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6974\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5000 - loss: 0.6972\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5000 - loss: 0.6970\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5000 - loss: 0.6967\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5000 - loss: 0.6965\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5000 - loss: 0.6962\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5000 - loss: 0.6960\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5000 - loss: 0.6957\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5000 - loss: 0.6955\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m keras_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mkeras_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m     18\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m keras_model\u001b[38;5;241m.\u001b[39mevaluate(X, y)\n",
      "File \u001b[1;32mc:\\Users\\ugo11\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ugo11\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    316\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menumerate_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ugo11\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:689\u001b[0m, in \u001b[0;36mTFEpochIterator.enumerate_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    687\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 689\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches:\n\u001b[0;32m    691\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\n\u001b[0;32m    692\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution\n\u001b[0;32m    693\u001b[0m         ):\n",
      "File \u001b[1;32mc:\\Users\\ugo11\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ugo11\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ugo11\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ugo11\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3509\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3508\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3509\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3510\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3512\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 📦 Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define MLP model in Keras\n",
    "keras_model = keras.Sequential([\n",
    "    keras.layers.Dense(4, activation='relu', input_shape=(2,)),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "keras_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "keras_model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = keras_model.evaluate(X, y)\n",
    "print(f\"Final Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 Advanced Topics in Neural Networks - Dropout, Batch Normalization & More\n",
    "\n",
    "Now that we've covered the basics of Neural Networks, let's dive into **advanced techniques** to improve performance:\n",
    "✔ **Dropout: Regularization to prevent overfitting**  \n",
    "✔ **Batch Normalization: Stabilizing training & speeding up convergence**  \n",
    "✔ **Weight Initialization: Preventing vanishing/exploding gradients**  \n",
    "✔ **Gradient Clipping: Handling exploding gradients**  \n",
    "✔ **Learning Rate Scheduling: Adaptive learning rates**  \n",
    "\n",
    "---\n",
    "\n",
    "# 📖 8. Dropout - Preventing Overfitting\n",
    "\n",
    "### 🔹 1. What is Dropout?\n",
    "\n",
    "**Dropout** is a **regularization technique** used to **prevent overfitting** by randomly dropping units (neurons) during training.\n",
    "\n",
    "### 🔹 2. How Dropout Works\n",
    "1. At each training step, randomly **drop neurons** with probability **$p$**.\n",
    "2. Forward pass continues **without these neurons**.\n",
    "3. At test time, **all neurons are active** but their outputs are scaled by **$1 - p$** to maintain consistency.\n",
    "\n",
    "### 🔹 3. Mathematical Formulation\n",
    "If **$h_i$** is the output of neuron **$i$**, then with dropout:\n",
    "\n",
    "$$\n",
    "h_i^{\\text{drop}} = \\frac{h_i}{1 - p} \\quad \\text{(at test time)}\n",
    "$$\n",
    "\n",
    "where **$p$** is the dropout probability.\n",
    "\n",
    "✅ **Why use Dropout?**\n",
    "- Reduces **co-adaptation** between neurons.\n",
    "- Acts as **ensemble learning** (averaging different sub-networks).\n",
    "- Helps **generalization** by forcing the model to be more robust.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Implementing Dropout in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define MLP model with Dropout\n",
    "class MLP_Dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # 50% neurons dropped\n",
    "        self.layer2 = nn.Linear(4, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.activation(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model_dropout = MLP_Dropout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Implementing Dropout in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Define MLP model with Dropout\n",
    "model_dropout = keras.Sequential([\n",
    "    keras.layers.Dense(4, activation='relu', input_shape=(2,)),\n",
    "    keras.layers.Dropout(0.5),  # Dropout layer with 50% probability\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_dropout.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Implementing Batch Normalization in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model with BatchNorm\n",
    "class MLP_BatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        self.batchnorm = nn.BatchNorm1d(4)  # Batch Normalization layer\n",
    "        self.layer2 = nn.Linear(4, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.batchnorm(x)  # Apply batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = self.activation(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model_batchnorm = MLP_BatchNorm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Implementing Batch Normalization in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Define MLP model with Batch Normalization\n",
    "model_batchnorm = keras.Sequential([\n",
    "    keras.layers.Dense(4, input_shape=(2,), activation='relu'),\n",
    "    keras.layers.BatchNormalization(),  # Batch Normalization layer\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_batchnorm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📖 9. Other Advanced Topics in Neural Networks\n",
    "\n",
    "In this section, we explore **advanced techniques** that improve training efficiency and stability:\n",
    "\n",
    "✔ **Weight Initialization:** Preventing vanishing/exploding gradients  \n",
    "✔ **Gradient Clipping:** Handling unstable gradients  \n",
    "✔ **Learning Rate Scheduling:** Adapting learning rates dynamically  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 1. Weight Initialization\n",
    "\n",
    "### 🔹 Why is Weight Initialization Important?\n",
    "- **Prevents vanishing/exploding gradients**  \n",
    "- **Speeds up convergence**  \n",
    "- **Ensures stable flow of activations through the network**  \n",
    "\n",
    "### 🔹 Xavier (Glorot) Initialization\n",
    "Used for **sigmoid & tanh activations**, ensures variance remains stable:\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{N}(0, \\frac{1}{n_{\\text{in}}})\n",
    "$$\n",
    "\n",
    "### 🔹 He Initialization\n",
    "Used for **ReLU & Leaky ReLU activations**, accounts for rectification:\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{N}(0, \\frac{2}{n_{\\text{in}}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Implementing Xavier & He Initialization in PyTorch\n",
    "\n",
    "# Xavier (Glorot) Initialization\n",
    "def xavier_init(shape):\n",
    "    return torch.randn(shape) * torch.sqrt(torch.tensor(1.0) / shape[1])\n",
    "\n",
    "# He Initialization (for ReLU networks)\n",
    "def he_init(shape):\n",
    "    return torch.randn(shape) * torch.sqrt(torch.tensor(2.0) / shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Implementing Xavier & He Initialization in Keras\n",
    "from tensorflow.keras.initializers import GlorotUniform, HeNormal\n",
    "\n",
    "# Xavier Initialization (Glorot)\n",
    "xavier_initializer = GlorotUniform()\n",
    "\n",
    "# He Initialization (for ReLU)\n",
    "he_initializer = HeNormal()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 2. Gradient Clipping\n",
    "\n",
    "### 🔹 Why Use Gradient Clipping?\n",
    "- **Prevents exploding gradients**, especially in deep networks or RNNs  \n",
    "- **Ensures stability** in weight updates  \n",
    "\n",
    "### 🔹 Gradient Clipping Formula\n",
    "\n",
    "For each weight **$w_i$**, if its gradient **$\\nabla w_i$** exceeds a threshold **$c$**, rescale:\n",
    "\n",
    "$$\n",
    "\\nabla w_i = \\frac{c}{||\\nabla w||} \\nabla w_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2987\n",
      "Epoch 2, Loss: 0.2903\n",
      "Epoch 3, Loss: 0.2819\n",
      "Epoch 4, Loss: 0.2737\n",
      "Epoch 5, Loss: 0.2655\n",
      "Epoch 6, Loss: 0.2574\n",
      "Epoch 7, Loss: 0.2495\n",
      "Epoch 8, Loss: 0.2417\n",
      "Epoch 9, Loss: 0.2339\n",
      "Epoch 10, Loss: 0.2264\n"
     ]
    }
   ],
   "source": [
    "# ✅ Implementing Gradient Clipping in PyTorch\n",
    "\n",
    "\n",
    "# Define a simple MLP model with Batch Normalization\n",
    "class MLP_BatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        self.batchnorm = nn.BatchNorm1d(4)  # Ensure this works correctly\n",
    "        self.layer2 = nn.Linear(4, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.batchnorm(x)  # Apply BatchNorm\n",
    "        x = torch.relu(x)\n",
    "        x = self.activation(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "torch_model = MLP_BatchNorm()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(torch_model.parameters(), lr=0.01)\n",
    "\n",
    "# ✅ Generate dummy batch data (Batch size = 2 or more)\n",
    "X_batch = torch.tensor([[0.5, 0.2], [0.1, 0.7]])  # Ensure batch size > 1\n",
    "\n",
    "# Training loop with Gradient Clipping\n",
    "for epoch in range(10):\n",
    "    torch_model.train()  # Ensure model is in training mode\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = torch_model(X_batch)  # Pass batch-sized input\n",
    "    loss = output.mean()  # Example loss function\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # ✅ Apply Gradient Clipping\n",
    "    utils.clip_grad_norm_(torch_model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Implementing Gradient Clipping in Keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define optimizer with gradient clipping\n",
    "optimizer = Adam(learning_rate=0.01, clipnorm=1.0)  # clipnorm prevents exploding gradients\n",
    "\n",
    "# Compile model with clipped gradients\n",
    "keras_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 3. Learning Rate Scheduling\n",
    "\n",
    "### 🔹 Why Use Learning Rate Scheduling?\n",
    "- **Adjusts learning rate dynamically** to **improve convergence**  \n",
    "- **Speeds up training** while avoiding overshooting  \n",
    "\n",
    "### 🔹 Common Learning Rate Schedulers\n",
    "\n",
    "#### **1. Exponential Decay**\n",
    "Gradually reduces the learning rate:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\alpha_0 \\times e^{-kt}\n",
    "$$\n",
    "\n",
    "#### **2. Step Decay**\n",
    "Reduces the learning rate **after a fixed number of steps**:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\alpha_0 \\times 0.5^{t//k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Learning Rate: 0.009000\n",
      "Epoch 2, Learning Rate: 0.008100\n",
      "Epoch 3, Learning Rate: 0.007290\n",
      "Epoch 4, Learning Rate: 0.006561\n",
      "Epoch 5, Learning Rate: 0.005905\n",
      "Epoch 6, Learning Rate: 0.005314\n",
      "Epoch 7, Learning Rate: 0.004783\n",
      "Epoch 8, Learning Rate: 0.004305\n",
      "Epoch 9, Learning Rate: 0.003874\n",
      "Epoch 10, Learning Rate: 0.003487\n",
      "Epoch 11, Learning Rate: 0.003138\n",
      "Epoch 12, Learning Rate: 0.002824\n",
      "Epoch 13, Learning Rate: 0.002542\n",
      "Epoch 14, Learning Rate: 0.002288\n",
      "Epoch 15, Learning Rate: 0.002059\n",
      "Epoch 16, Learning Rate: 0.001853\n",
      "Epoch 17, Learning Rate: 0.001668\n",
      "Epoch 18, Learning Rate: 0.001501\n",
      "Epoch 19, Learning Rate: 0.001351\n",
      "Epoch 20, Learning Rate: 0.001216\n"
     ]
    }
   ],
   "source": [
    "# ✅ Implementing Learning Rate Scheduling in PyTorch\n",
    "\n",
    "# Define MLP model with Batch Normalization\n",
    "class MLP_BatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        self.batchnorm = nn.BatchNorm1d(4)  # Ensure batch size > 1\n",
    "        self.layer2 = nn.Linear(4, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.batchnorm(x)  # Apply BatchNorm\n",
    "        x = torch.relu(x)\n",
    "        x = self.activation(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "torch_model = MLP_BatchNorm()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(torch_model.parameters(), lr=0.01)\n",
    "\n",
    "# Exponential Decay\n",
    "scheduler_exp = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "# Step Decay\n",
    "scheduler_step = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# ✅ Generate dummy batch data (Batch size = 2 or more)\n",
    "X_batch = torch.tensor([[0.5, 0.2], [0.1, 0.7]])  # Ensure batch size > 1\n",
    "\n",
    "# Training loop with Learning Rate Scheduling\n",
    "for epoch in range(20):\n",
    "    torch_model.train()  # Ensure model is in training mode\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = torch_model(X_batch)  # Pass batch-sized input\n",
    "    loss = output.mean()  # Example loss function\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # ✅ Apply Gradient Clipping\n",
    "    utils.clip_grad_norm_(torch_model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    # ✅ Update learning rate\n",
    "    scheduler_exp.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Learning Rate: {scheduler_exp.get_last_lr()[0]:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.7500 - loss: 0.6492 - learning_rate: 0.0090\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6471 - learning_rate: 0.0082\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7500 - loss: 0.6434 - learning_rate: 0.0074\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7500 - loss: 0.6405 - learning_rate: 0.0067\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7500 - loss: 0.6384 - learning_rate: 0.0061\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7500 - loss: 0.6364 - learning_rate: 0.0055\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7500 - loss: 0.6346 - learning_rate: 0.0050\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7500 - loss: 0.6330 - learning_rate: 0.0045\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7500 - loss: 0.6315 - learning_rate: 0.0041\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7500 - loss: 0.6301 - learning_rate: 0.0037\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7500 - loss: 0.6288 - learning_rate: 0.0033\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7500 - loss: 0.6276 - learning_rate: 0.0030\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7500 - loss: 0.6266 - learning_rate: 0.0027\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.7500 - loss: 0.6257 - learning_rate: 0.0025\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7500 - loss: 0.6249 - learning_rate: 0.0022\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7500 - loss: 0.6242 - learning_rate: 0.0020\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7500 - loss: 0.6235 - learning_rate: 0.0018\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7500 - loss: 0.6229 - learning_rate: 0.0017\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7500 - loss: 0.6223 - learning_rate: 0.0015\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7500 - loss: 0.6218 - learning_rate: 0.0014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x201cc518230>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ Implementing Learning Rate Scheduling in Keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Define exponential decay function\n",
    "def exponential_decay(epoch, lr):\n",
    "    return lr * np.exp(-0.1)\n",
    "\n",
    "# Define step decay function\n",
    "def step_decay(epoch, lr):\n",
    "    if epoch % 10 == 0:\n",
    "        return lr * 0.5\n",
    "    return lr\n",
    "\n",
    "# Apply scheduler callbacks\n",
    "exp_decay_callback = LearningRateScheduler(exponential_decay)\n",
    "step_decay_callback = LearningRateScheduler(step_decay)\n",
    "\n",
    "# Train model with learning rate scheduler\n",
    "keras_model.fit(X, y, epochs=20, callbacks=[exp_decay_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 10. Interview Questions\n",
    "\n",
    "### 🔹 **Dropout Questions**\n",
    "1️⃣ **Why does Dropout help prevent overfitting?**  \n",
    "   - Forces the network to learn redundant representations.  \n",
    "   \n",
    "2️⃣ **How does Dropout affect training vs. inference?**  \n",
    "   - During training, neurons are randomly dropped with probability **p**.  \n",
    "   - During inference, all neurons are active, but their outputs are scaled by **(1 - p)** to maintain consistency.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Batch Normalization Questions**\n",
    "3️⃣ **Why does BatchNorm improve training?**  \n",
    "   - It stabilizes learning, reduces internal covariate shift, and allows for higher learning rates.  \n",
    "\n",
    "4️⃣ **How does BatchNorm act as a regularization technique?**  \n",
    "   - It introduces slight noise to the activations, similar to Dropout, which reduces overfitting.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Optimization Questions**\n",
    "5️⃣ **Why use Xavier (Glorot) initialization?**  \n",
    "   - It ensures that the variance of activations remains stable across layers, preventing vanishing/exploding gradients.  \n",
    "\n",
    "6️⃣ **Why use He initialization for ReLU networks?**  \n",
    "   - He initialization scales weights to maintain variance in ReLU-based networks, improving convergence.  \n",
    "\n",
    "7️⃣ **What is the purpose of Gradient Clipping?**  \n",
    "   - It prevents exploding gradients by capping their magnitude during backpropagation.  \n",
    "\n",
    "8️⃣ **When should you use Learning Rate Scheduling?**  \n",
    "   - When training loss plateaus, reducing the learning rate helps the model converge better.  \n",
    "\n",
    "9️⃣ **What are the different types of Learning Rate Schedulers?**  \n",
    "   - **Exponential Decay:** Decreases learning rate exponentially over time.  \n",
    "   - **Step Decay:** Reduces learning rate after fixed steps.  \n",
    "   - **Adaptive Methods (Adam, RMSprop):** Adjusts learning rates dynamically based on past gradients.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Weight Initialization Questions**\n",
    "1️⃣ **Why is weight initialization important?**  \n",
    "   - Prevents vanishing/exploding gradients.  \n",
    "\n",
    "2️⃣ **When should you use Xavier initialization?**  \n",
    "   - For **sigmoid and tanh** activation functions.  \n",
    "\n",
    "3️⃣ **When should you use He initialization?**  \n",
    "   - For **ReLU and Leaky ReLU** activations.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Gradient Clipping Questions**\n",
    "4️⃣ **Why do we clip gradients?**  \n",
    "   - To prevent **exploding gradients**, especially in deep networks and RNNs.  \n",
    "\n",
    "5️⃣ **What is the difference between `clipnorm` and `clipvalue`?**  \n",
    "   - **`clipnorm`** rescales gradients if their **L2 norm exceeds a threshold**.  \n",
    "   - **`clipvalue`** clips each gradient component **individually**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Learning Rate Scheduling Questions**\n",
    "6️⃣ **Why use a dynamic learning rate instead of a fixed one?**  \n",
    "   - **Higher learning rates** speed up training initially.  \n",
    "   - **Lower learning rates** improve convergence at later stages.  \n",
    "\n",
    "7️⃣ **What’s the difference between Exponential Decay and Step Decay?**  \n",
    "   - **Exponential Decay** gradually decreases learning rate every step.  \n",
    "   - **Step Decay** reduces learning rate **at fixed intervals**.  \n",
    "\n",
    "8️⃣ **When should you decay the learning rate?**  \n",
    "   - When the loss **plateaus** and is no longer improving.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
