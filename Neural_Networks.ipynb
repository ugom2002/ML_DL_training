{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 Neural Networks (Basics) - A Complete Guide\n",
    "\n",
    "This notebook provides a **detailed breakdown** of fundamental neural network concepts, including:  \n",
    "✔ **Perceptron & Multi-layer Perceptron (MLP)**  \n",
    "✔ **Activation Functions & Their Derivatives**  \n",
    "✔ **Forward & Backpropagation with Detailed Math**  \n",
    "✔ **Gradient Descent for Neural Networks**  \n",
    "✔ **Implementation in PyTorch & Keras**\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 1. Introduction to Neural Networks\n",
    "\n",
    "Neural Networks are inspired by the **human brain** and consist of **layers of neurons** that transform input data through weighted connections.  \n",
    "\n",
    "### 🔹 Why Use Neural Networks?\n",
    "✔ **Can model complex relationships** between inputs and outputs.  \n",
    "✔ **Can approximate any function (Universal Approximation Theorem)**.  \n",
    "✔ **Used in deep learning applications like NLP, Vision, and Reinforcement Learning**.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 2. The Perceptron: The Basic Building Block\n",
    "\n",
    "### 🔹 1. The Perceptron Model  \n",
    "A perceptron is a **single-layer neural network** that performs **binary classification**:\n",
    "\n",
    "$$\n",
    "y = f(W \\cdot X + b)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **$X$** = Input vector\n",
    "- **$W$** = Weights\n",
    "- **$b$** = Bias\n",
    "- **$f$** = Activation function\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Perceptron Learning Rule\n",
    "\n",
    "1. Initialize weights randomly.  \n",
    "2. Compute the **weighted sum** of inputs:  \n",
    "\n",
    "   $$\n",
    "   z = W \\cdot X + b\n",
    "   $$\n",
    "\n",
    "3. Apply an **activation function** (e.g., step function):\n",
    "\n",
    "   $$\n",
    "   y = \\text{sign}(z)\n",
    "   $$\n",
    "\n",
    "4. **Update weights** using the perceptron update rule:\n",
    "\n",
    "   $$\n",
    "   W := W + \\alpha (y_{\\text{true}} - y_{\\text{pred}}) X\n",
    "   $$\n",
    "\n",
    "5. Repeat until convergence.\n",
    "\n",
    "✅ **Limitations**:  \n",
    "- Can only solve **linearly separable** problems (e.g., AND, OR).\n",
    "- **Cannot solve XOR** → Requires Multi-Layer Perceptrons (MLPs).\n",
    "\n",
    "---\n",
    "\n",
    "# 📖 3. Multi-Layer Perceptron (MLP)\n",
    "\n",
    "A **Multi-Layer Perceptron (MLP)** consists of:\n",
    "- **Input Layer**\n",
    "- **Hidden Layers** (with non-linear activations)\n",
    "- **Output Layer**\n",
    "\n",
    "Each neuron in layer **$l$** receives input **$a^{(l-1)}$** from the previous layer:\n",
    "\n",
    "$$\n",
    "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{(l)} = f(z^{(l)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **$W^{(l)}$** = Weights of layer **$l$**  \n",
    "- **$b^{(l)}$** = Bias  \n",
    "- **$f$** = Activation function  \n",
    "\n",
    "✅ **MLPs can learn complex patterns, including XOR**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 4. Activation Functions & Their Derivatives\n",
    "\n",
    "### 🔹 1. Sigmoid Function\n",
    "Used in binary classification:\n",
    "\n",
    "$$\n",
    "f(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Derivative:\n",
    "\n",
    "$$\n",
    "f'(z) = f(z) (1 - f(z))\n",
    "$$\n",
    "\n",
    "✅ **Smooth, differentiable**  \n",
    "⚠ **Suffers from vanishing gradients**  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. ReLU (Rectified Linear Unit)\n",
    "Most popular in deep networks:\n",
    "\n",
    "$$\n",
    "f(z) = \\max(0, z)\n",
    "$$\n",
    "\n",
    "Derivative:\n",
    "\n",
    "$$\n",
    "f'(z) =\n",
    "\\begin{cases}\n",
    "1, & z > 0 \\\\\n",
    "0, & z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "✅ **Faster convergence**  \n",
    "⚠ **Can cause dead neurons (ReLU dying problem)**  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 3. Softmax Function\n",
    "Used in multi-class classification:\n",
    "\n",
    "$$\n",
    "f(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial z_i} = f(z_i) (1 - f(z_i))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 5. Backpropagation: Training Neural Networks\n",
    "\n",
    "To train an MLP, we use **Backpropagation**, which updates weights based on the **chain rule of calculus**.\n",
    "\n",
    "### 🔹 1. Compute Loss Function\n",
    "For **binary classification**, we use **Binary Cross-Entropy**:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "For **multi-class classification**, we use **Categorical Cross-Entropy**:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\sum_{i=1}^{m} \\sum_{j=1}^{k} y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Compute Gradients Using Chain Rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W} = \\frac{\\partial J}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 6. Implementing Neural Networks in PyTorch and Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 1. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features (4 attributes)\n",
    "y = iris.target  # Labels (0, 1, 2)\n",
    "\n",
    "# ✅ 2. Preprocess the data\n",
    "scaler = StandardScaler()  # Standardize the features\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)  # Multi-class classification\n",
    "\n",
    "# ✅ 3. Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ 4. Create DataLoader for batch training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# ✅ 5. Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(4, 16)  # 4 input features -> 16 hidden neurons\n",
    "        self.layer2 = nn.Linear(16, 8)  # 16 -> 8 hidden neurons\n",
    "        self.layer3 = nn.Linear(8, 3)   # 8 -> 3 output classes (softmax not needed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)  # No softmax here since CrossEntropyLoss handles it\n",
    "        return x\n",
    "\n",
    "# ✅ 6. Initialize model, loss function, and optimizer\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ✅ 7. Train the model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ✅ 8. Evaluate the model\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        _, predicted = torch.max(y_pred, 1)  # Select the class with the highest probability\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"📊 Accuracy on the test set: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 7. Implementing Neural Networks in Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ugo11\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.4547 - loss: 1.0083 - val_accuracy: 0.6000 - val_loss: 0.9759\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4957 - loss: 0.9502 - val_accuracy: 0.6000 - val_loss: 0.9480\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4585 - loss: 0.9833 - val_accuracy: 0.6000 - val_loss: 0.9228\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5017 - loss: 0.9301 - val_accuracy: 0.7000 - val_loss: 0.8973\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5089 - loss: 0.9355 - val_accuracy: 0.7333 - val_loss: 0.8723\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6684 - loss: 0.9078 - val_accuracy: 0.7333 - val_loss: 0.8459\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7164 - loss: 0.8785 - val_accuracy: 0.8333 - val_loss: 0.8199\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7365 - loss: 0.8751 - val_accuracy: 0.8667 - val_loss: 0.7932\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7486 - loss: 0.8455 - val_accuracy: 0.9000 - val_loss: 0.7647\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7971 - loss: 0.8132 - val_accuracy: 0.9000 - val_loss: 0.7329\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7618 - loss: 0.8156 - val_accuracy: 0.9000 - val_loss: 0.7008\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7458 - loss: 0.7833 - val_accuracy: 0.9000 - val_loss: 0.6680\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7780 - loss: 0.7221 - val_accuracy: 0.9000 - val_loss: 0.6327\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7615 - loss: 0.7190 - val_accuracy: 0.9000 - val_loss: 0.5985\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7925 - loss: 0.6676 - val_accuracy: 0.9000 - val_loss: 0.5642\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8426 - loss: 0.5978 - val_accuracy: 0.9000 - val_loss: 0.5291\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7892 - loss: 0.6266 - val_accuracy: 0.9000 - val_loss: 0.4990\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7501 - loss: 0.6247 - val_accuracy: 0.9000 - val_loss: 0.4699\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7658 - loss: 0.5397 - val_accuracy: 0.9000 - val_loss: 0.4421\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7847 - loss: 0.5352 - val_accuracy: 0.9000 - val_loss: 0.4188\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8130 - loss: 0.4859 - val_accuracy: 0.9000 - val_loss: 0.3974\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8465 - loss: 0.4567 - val_accuracy: 0.9000 - val_loss: 0.3785\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8555 - loss: 0.4206 - val_accuracy: 0.9000 - val_loss: 0.3617\n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8439 - loss: 0.4217 - val_accuracy: 0.9000 - val_loss: 0.3474\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7971 - loss: 0.4078 - val_accuracy: 0.9000 - val_loss: 0.3352\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8536 - loss: 0.4017 - val_accuracy: 0.9000 - val_loss: 0.3249\n",
      "Epoch 27/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8192 - loss: 0.4073 - val_accuracy: 0.9000 - val_loss: 0.3159\n",
      "Epoch 28/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8252 - loss: 0.3662 - val_accuracy: 0.9000 - val_loss: 0.3062\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8371 - loss: 0.3890 - val_accuracy: 0.9000 - val_loss: 0.2977\n",
      "Epoch 30/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8279 - loss: 0.3876 - val_accuracy: 0.9000 - val_loss: 0.2891\n",
      "Epoch 31/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8594 - loss: 0.3349 - val_accuracy: 0.9000 - val_loss: 0.2800\n",
      "Epoch 32/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8140 - loss: 0.3915 - val_accuracy: 0.9000 - val_loss: 0.2722\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8433 - loss: 0.3466 - val_accuracy: 0.9000 - val_loss: 0.2652\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8513 - loss: 0.3202 - val_accuracy: 0.9000 - val_loss: 0.2586\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8966 - loss: 0.2946 - val_accuracy: 0.9333 - val_loss: 0.2503\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8675 - loss: 0.3238 - val_accuracy: 0.9333 - val_loss: 0.2427\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9353 - loss: 0.2660 - val_accuracy: 0.9333 - val_loss: 0.2351\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9149 - loss: 0.2974 - val_accuracy: 0.9333 - val_loss: 0.2285\n",
      "Epoch 39/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8856 - loss: 0.3189 - val_accuracy: 0.9333 - val_loss: 0.2232\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9134 - loss: 0.2718 - val_accuracy: 0.9000 - val_loss: 0.2178\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9044 - loss: 0.2569 - val_accuracy: 0.9000 - val_loss: 0.2123\n",
      "Epoch 42/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9271 - loss: 0.2522 - val_accuracy: 0.9333 - val_loss: 0.2062\n",
      "Epoch 43/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9122 - loss: 0.2562 - val_accuracy: 0.9667 - val_loss: 0.1999\n",
      "Epoch 44/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9577 - loss: 0.2198 - val_accuracy: 0.9667 - val_loss: 0.1938\n",
      "Epoch 45/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9546 - loss: 0.2348 - val_accuracy: 0.9667 - val_loss: 0.1898\n",
      "Epoch 46/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9673 - loss: 0.2181 - val_accuracy: 0.9667 - val_loss: 0.1830\n",
      "Epoch 47/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9390 - loss: 0.2389 - val_accuracy: 0.9667 - val_loss: 0.1788\n",
      "Epoch 48/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9572 - loss: 0.2183 - val_accuracy: 0.9667 - val_loss: 0.1736\n",
      "Epoch 49/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9497 - loss: 0.2041 - val_accuracy: 0.9667 - val_loss: 0.1694\n",
      "Epoch 50/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9381 - loss: 0.2449 - val_accuracy: 0.9667 - val_loss: 0.1639\n",
      "Epoch 51/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9383 - loss: 0.2250 - val_accuracy: 1.0000 - val_loss: 0.1585\n",
      "Epoch 52/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9413 - loss: 0.2234 - val_accuracy: 1.0000 - val_loss: 0.1540\n",
      "Epoch 53/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9394 - loss: 0.1968 - val_accuracy: 1.0000 - val_loss: 0.1504\n",
      "Epoch 54/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9514 - loss: 0.1992 - val_accuracy: 1.0000 - val_loss: 0.1443\n",
      "Epoch 55/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9565 - loss: 0.1805 - val_accuracy: 1.0000 - val_loss: 0.1422\n",
      "Epoch 56/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9539 - loss: 0.1906 - val_accuracy: 1.0000 - val_loss: 0.1388\n",
      "Epoch 57/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9568 - loss: 0.1661 - val_accuracy: 1.0000 - val_loss: 0.1341\n",
      "Epoch 58/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9554 - loss: 0.1746 - val_accuracy: 1.0000 - val_loss: 0.1301\n",
      "Epoch 59/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9355 - loss: 0.2030 - val_accuracy: 1.0000 - val_loss: 0.1262\n",
      "Epoch 60/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9476 - loss: 0.1593 - val_accuracy: 1.0000 - val_loss: 0.1231\n",
      "Epoch 61/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9490 - loss: 0.1842 - val_accuracy: 1.0000 - val_loss: 0.1184\n",
      "Epoch 62/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9479 - loss: 0.1679 - val_accuracy: 1.0000 - val_loss: 0.1156\n",
      "Epoch 63/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9593 - loss: 0.1364 - val_accuracy: 1.0000 - val_loss: 0.1141\n",
      "Epoch 64/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9479 - loss: 0.1641 - val_accuracy: 1.0000 - val_loss: 0.1103\n",
      "Epoch 65/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9355 - loss: 0.1505 - val_accuracy: 1.0000 - val_loss: 0.1063\n",
      "Epoch 66/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9371 - loss: 0.1464 - val_accuracy: 1.0000 - val_loss: 0.1048\n",
      "Epoch 67/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9390 - loss: 0.1446 - val_accuracy: 1.0000 - val_loss: 0.1025\n",
      "Epoch 68/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9243 - loss: 0.1463 - val_accuracy: 1.0000 - val_loss: 0.1020\n",
      "Epoch 69/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9472 - loss: 0.1479 - val_accuracy: 1.0000 - val_loss: 0.0984\n",
      "Epoch 70/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9462 - loss: 0.1359 - val_accuracy: 1.0000 - val_loss: 0.0997\n",
      "Epoch 71/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9487 - loss: 0.1519 - val_accuracy: 1.0000 - val_loss: 0.0996\n",
      "Epoch 72/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9463 - loss: 0.1434 - val_accuracy: 1.0000 - val_loss: 0.0965\n",
      "Epoch 73/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9509 - loss: 0.1407 - val_accuracy: 1.0000 - val_loss: 0.0928\n",
      "Epoch 74/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9471 - loss: 0.1265 - val_accuracy: 1.0000 - val_loss: 0.0913\n",
      "Epoch 75/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9471 - loss: 0.1255 - val_accuracy: 1.0000 - val_loss: 0.0897\n",
      "Epoch 76/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9391 - loss: 0.1191 - val_accuracy: 1.0000 - val_loss: 0.0878\n",
      "Epoch 77/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9521 - loss: 0.1232 - val_accuracy: 1.0000 - val_loss: 0.0864\n",
      "Epoch 78/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9407 - loss: 0.1196 - val_accuracy: 1.0000 - val_loss: 0.0862\n",
      "Epoch 79/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9513 - loss: 0.1238 - val_accuracy: 1.0000 - val_loss: 0.0843\n",
      "Epoch 80/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9569 - loss: 0.1136 - val_accuracy: 1.0000 - val_loss: 0.0823\n",
      "Epoch 81/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9427 - loss: 0.1340 - val_accuracy: 1.0000 - val_loss: 0.0803\n",
      "Epoch 82/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9738 - loss: 0.0912 - val_accuracy: 1.0000 - val_loss: 0.0791\n",
      "Epoch 83/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9523 - loss: 0.1261 - val_accuracy: 1.0000 - val_loss: 0.0779\n",
      "Epoch 84/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9341 - loss: 0.1118 - val_accuracy: 1.0000 - val_loss: 0.0772\n",
      "Epoch 85/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9496 - loss: 0.1065 - val_accuracy: 1.0000 - val_loss: 0.0773\n",
      "Epoch 86/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9733 - loss: 0.0823 - val_accuracy: 1.0000 - val_loss: 0.0767\n",
      "Epoch 87/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9633 - loss: 0.0944 - val_accuracy: 1.0000 - val_loss: 0.0743\n",
      "Epoch 88/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9297 - loss: 0.1199 - val_accuracy: 1.0000 - val_loss: 0.0732\n",
      "Epoch 89/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9299 - loss: 0.1139 - val_accuracy: 1.0000 - val_loss: 0.0723\n",
      "Epoch 90/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9562 - loss: 0.1018 - val_accuracy: 1.0000 - val_loss: 0.0713\n",
      "Epoch 91/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9415 - loss: 0.1150 - val_accuracy: 1.0000 - val_loss: 0.0717\n",
      "Epoch 92/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9533 - loss: 0.1004 - val_accuracy: 1.0000 - val_loss: 0.0712\n",
      "Epoch 93/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9608 - loss: 0.0972 - val_accuracy: 1.0000 - val_loss: 0.0694\n",
      "Epoch 94/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9842 - loss: 0.0848 - val_accuracy: 1.0000 - val_loss: 0.0696\n",
      "Epoch 95/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9752 - loss: 0.0680 - val_accuracy: 1.0000 - val_loss: 0.0680\n",
      "Epoch 96/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9559 - loss: 0.0973 - val_accuracy: 1.0000 - val_loss: 0.0662\n",
      "Epoch 97/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9354 - loss: 0.1041 - val_accuracy: 1.0000 - val_loss: 0.0654\n",
      "Epoch 98/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9691 - loss: 0.0811 - val_accuracy: 1.0000 - val_loss: 0.0655\n",
      "Epoch 99/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9784 - loss: 0.0691 - val_accuracy: 1.0000 - val_loss: 0.0651\n",
      "Epoch 100/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9585 - loss: 0.0960 - val_accuracy: 1.0000 - val_loss: 0.0684\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.0684\n",
      "📊 Final Loss: 0.0684, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 📦 Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 1. Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # 4 features\n",
    "y = iris.target  # 3 classes (0, 1, 2)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# ✅ One-Hot Encoding \n",
    "encoder = OneHotEncoder(sparse_output=False)  # Required for Keras since 'categorical_crossentropy' does not handle class indices directly\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))  # Convert labels to one-hot encoded format\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ 2. Define the MLP model in Keras\n",
    "keras_model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation='relu', input_shape=(4,)),  # 4 inputs → 16 neurons\n",
    "    keras.layers.Dense(8, activation='relu'),  # 8 hidden neurons\n",
    "    keras.layers.Dense(3, activation='softmax')  # 3 output classes → softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# ✅ 3. Compile the model\n",
    "keras_model.compile(optimizer='adam', \n",
    "                    loss='categorical_crossentropy',  # CrossEntropy for multi-class classification\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# ✅ 4. Train the model\n",
    "keras_model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# ✅ 5. Evaluate the model\n",
    "loss, accuracy = keras_model.evaluate(X_test, y_test)\n",
    "print(f\"📊 Final Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 Advanced Topics in Neural Networks - Dropout, Batch Normalization & More\n",
    "\n",
    "Now that we've covered the basics of Neural Networks, let's dive into **advanced techniques** to improve performance:\n",
    "✔ **Dropout: Regularization to prevent overfitting**  \n",
    "✔ **Batch Normalization: Stabilizing training & speeding up convergence**  \n",
    "✔ **Weight Initialization: Preventing vanishing/exploding gradients**  \n",
    "✔ **Gradient Clipping: Handling exploding gradients**  \n",
    "✔ **Learning Rate Scheduling: Adaptive learning rates**  \n",
    "\n",
    "---\n",
    "\n",
    "# 📖 8. Dropout - Preventing Overfitting\n",
    "\n",
    "### 🔹 1. What is Dropout?\n",
    "\n",
    "**Dropout** is a **regularization technique** used to **prevent overfitting** by randomly dropping units (neurons) during training.\n",
    "\n",
    "### 🔹 2. How Dropout Works\n",
    "1. At each training step, randomly **drop neurons** with probability **$p$**.\n",
    "2. Forward pass continues **without these neurons**.\n",
    "3. At test time, **all neurons are active** but their outputs are scaled by **$1 - p$** to maintain consistency.\n",
    "\n",
    "### 🔹 3. Mathematical Formulation\n",
    "If **$h_i$** is the output of neuron **$i$**, then with dropout:\n",
    "\n",
    "$$\n",
    "h_i^{\\text{drop}} = \\frac{h_i}{1 - p} \\quad \\text{(at test time)}\n",
    "$$\n",
    "\n",
    "where **$p$** is the dropout probability.\n",
    "\n",
    "✅ **Why use Dropout?**\n",
    "- Reduces **co-adaptation** between neurons.\n",
    "- Acts as **ensemble learning** (averaging different sub-networks).\n",
    "- Helps **generalization** by forcing the model to be more robust.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Implementing Dropout in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define MLP model with Dropout\n",
    "class MLP_Dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # 50% neurons dropped\n",
    "        self.layer2 = nn.Linear(4, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.activation(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model_dropout = MLP_Dropout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Implementing Dropout in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Define MLP model with Dropout\n",
    "model_dropout = keras.Sequential([\n",
    "    keras.layers.Dense(4, activation='relu', input_shape=(2,)),\n",
    "    keras.layers.Dropout(0.5),  # Dropout layer with 50% probability\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_dropout.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Implementing Batch Normalization in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model with BatchNorm\n",
    "class MLP_BatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        self.batchnorm = nn.BatchNorm1d(4)  # Batch Normalization layer\n",
    "        self.layer2 = nn.Linear(4, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.batchnorm(x)  # Apply batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = self.activation(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model_batchnorm = MLP_BatchNorm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Implementing Batch Normalization in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Define MLP model with Batch Normalization\n",
    "model_batchnorm = keras.Sequential([\n",
    "    keras.layers.Dense(4, input_shape=(2,), activation='relu'),\n",
    "    keras.layers.BatchNormalization(),  # Batch Normalization layer\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_batchnorm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📖 9. Other Advanced Topics in Neural Networks\n",
    "\n",
    "In this section, we explore **advanced techniques** that improve training efficiency and stability:\n",
    "\n",
    "✔ **Weight Initialization:** Preventing vanishing/exploding gradients  \n",
    "✔ **Gradient Clipping:** Handling unstable gradients  \n",
    "✔ **Learning Rate Scheduling:** Adapting learning rates dynamically  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 1. Weight Initialization\n",
    "\n",
    "### 🔹 Why is Weight Initialization Important?\n",
    "- **Prevents vanishing/exploding gradients**  \n",
    "- **Speeds up convergence**  \n",
    "- **Ensures stable flow of activations through the network**  \n",
    "\n",
    "### 🔹 Xavier (Glorot) Initialization\n",
    "Used for **sigmoid & tanh activations**, ensures variance remains stable:\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{N}(0, \\frac{1}{n_{\\text{in}}})\n",
    "$$\n",
    "\n",
    "### 🔹 He Initialization\n",
    "Used for **ReLU & Leaky ReLU activations**, accounts for rectification:\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{N}(0, \\frac{2}{n_{\\text{in}}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Implementing Xavier & He Initialization in PyTorch\n",
    "\n",
    "# Xavier (Glorot) Initialization\n",
    "def xavier_init(shape):\n",
    "    return torch.randn(shape) * torch.sqrt(torch.tensor(1.0) / shape[1])\n",
    "\n",
    "# He Initialization (for ReLU networks)\n",
    "def he_init(shape):\n",
    "    return torch.randn(shape) * torch.sqrt(torch.tensor(2.0) / shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Implementing Xavier & He Initialization in Keras\n",
    "from tensorflow.keras.initializers import GlorotUniform, HeNormal\n",
    "\n",
    "# Xavier Initialization (Glorot)\n",
    "xavier_initializer = GlorotUniform()\n",
    "\n",
    "# He Initialization (for ReLU)\n",
    "he_initializer = HeNormal()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 2. Gradient Clipping\n",
    "\n",
    "### 🔹 Why Use Gradient Clipping?\n",
    "- **Prevents exploding gradients**, especially in deep networks or RNNs  \n",
    "- **Ensures stability** in weight updates  \n",
    "\n",
    "### 🔹 Gradient Clipping Formula\n",
    "\n",
    "For each weight **$w_i$**, if its gradient **$\\nabla w_i$** exceeds a threshold **$c$**, rescale:\n",
    "\n",
    "$$\n",
    "\\nabla w_i = \\frac{c}{||\\nabla w||} \\nabla w_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.0115\n",
      "Epoch 20/100, Loss: 0.0015\n",
      "Epoch 30/100, Loss: 0.0011\n",
      "Epoch 40/100, Loss: 0.0287\n",
      "Epoch 50/100, Loss: 0.0063\n",
      "Epoch 60/100, Loss: 0.0004\n",
      "Epoch 70/100, Loss: 0.0006\n",
      "Epoch 80/100, Loss: 0.0002\n",
      "Epoch 90/100, Loss: 0.0006\n",
      "Epoch 100/100, Loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "# ✅ Training the model with Gradient Clipping\n",
    "epochs = 100\n",
    "clip_value = 1.0  # Define the maximum gradient norm\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # 🚀 Apply Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_value)\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.4025 - loss: 1.0209 - val_accuracy: 0.5667 - val_loss: 0.8582\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5898 - loss: 0.8453 - val_accuracy: 0.8000 - val_loss: 0.6945\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7903 - loss: 0.6648 - val_accuracy: 0.8333 - val_loss: 0.5211\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8339 - loss: 0.4924 - val_accuracy: 0.8667 - val_loss: 0.3221\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8487 - loss: 0.3645 - val_accuracy: 0.9333 - val_loss: 0.2132\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9068 - loss: 0.2288 - val_accuracy: 0.9667 - val_loss: 0.1742\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9524 - loss: 0.1634 - val_accuracy: 0.9667 - val_loss: 0.1612\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9582 - loss: 0.1732 - val_accuracy: 0.9667 - val_loss: 0.1300\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9312 - loss: 0.1546 - val_accuracy: 1.0000 - val_loss: 0.1050\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9507 - loss: 0.1235 - val_accuracy: 1.0000 - val_loss: 0.0705\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9415 - loss: 0.1132 - val_accuracy: 1.0000 - val_loss: 0.0652\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9720 - loss: 0.1079 - val_accuracy: 0.9667 - val_loss: 0.0764\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9442 - loss: 0.0980 - val_accuracy: 0.9667 - val_loss: 0.0636\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9549 - loss: 0.0862 - val_accuracy: 0.9667 - val_loss: 0.0694\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9740 - loss: 0.0734 - val_accuracy: 0.9667 - val_loss: 0.0592\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9842 - loss: 0.0673 - val_accuracy: 1.0000 - val_loss: 0.0553\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9842 - loss: 0.0570 - val_accuracy: 1.0000 - val_loss: 0.0502\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9745 - loss: 0.0687 - val_accuracy: 1.0000 - val_loss: 0.0413\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9946 - loss: 0.0540 - val_accuracy: 0.9667 - val_loss: 0.0530\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9866 - loss: 0.0566 - val_accuracy: 0.9667 - val_loss: 0.0541\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9733 - loss: 0.0454 - val_accuracy: 1.0000 - val_loss: 0.0460\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9795 - loss: 0.0446 - val_accuracy: 1.0000 - val_loss: 0.0429\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9748 - loss: 0.0526 - val_accuracy: 0.9667 - val_loss: 0.0459\n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9735 - loss: 0.0538 - val_accuracy: 1.0000 - val_loss: 0.0419\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9875 - loss: 0.0478 - val_accuracy: 1.0000 - val_loss: 0.0328\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9852 - loss: 0.0389 - val_accuracy: 1.0000 - val_loss: 0.0226\n",
      "Epoch 27/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9920 - loss: 0.0329 - val_accuracy: 1.0000 - val_loss: 0.0253\n",
      "Epoch 28/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9761 - loss: 0.0528 - val_accuracy: 1.0000 - val_loss: 0.0228\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9857 - loss: 0.0364 - val_accuracy: 1.0000 - val_loss: 0.0229\n",
      "Epoch 30/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9730 - loss: 0.0684 - val_accuracy: 1.0000 - val_loss: 0.0242\n",
      "Epoch 31/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9707 - loss: 0.0713 - val_accuracy: 1.0000 - val_loss: 0.0197\n",
      "Epoch 32/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9981 - loss: 0.0354 - val_accuracy: 1.0000 - val_loss: 0.0234\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9776 - loss: 0.0494 - val_accuracy: 1.0000 - val_loss: 0.0242\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9981 - loss: 0.0223 - val_accuracy: 1.0000 - val_loss: 0.0189\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9777 - loss: 0.0429 - val_accuracy: 1.0000 - val_loss: 0.0162\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9817 - loss: 0.0496 - val_accuracy: 1.0000 - val_loss: 0.0169\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9906 - loss: 0.0387 - val_accuracy: 1.0000 - val_loss: 0.0189\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9730 - loss: 0.0608 - val_accuracy: 1.0000 - val_loss: 0.0164\n",
      "Epoch 39/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9834 - loss: 0.0353 - val_accuracy: 1.0000 - val_loss: 0.0152\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9900 - loss: 0.0376 - val_accuracy: 1.0000 - val_loss: 0.0143\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9672 - loss: 0.0610 - val_accuracy: 1.0000 - val_loss: 0.0156\n",
      "Epoch 42/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9875 - loss: 0.0367 - val_accuracy: 1.0000 - val_loss: 0.0137\n",
      "Epoch 43/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9932 - loss: 0.0253 - val_accuracy: 1.0000 - val_loss: 0.0133\n",
      "Epoch 44/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9972 - loss: 0.0243 - val_accuracy: 1.0000 - val_loss: 0.0146\n",
      "Epoch 45/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9773 - loss: 0.0542 - val_accuracy: 1.0000 - val_loss: 0.0102\n",
      "Epoch 46/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9981 - loss: 0.0268 - val_accuracy: 1.0000 - val_loss: 0.0141\n",
      "Epoch 47/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9707 - loss: 0.0661 - val_accuracy: 1.0000 - val_loss: 0.0140\n",
      "Epoch 48/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9900 - loss: 0.0366 - val_accuracy: 1.0000 - val_loss: 0.0110\n",
      "Epoch 49/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9812 - loss: 0.0345 - val_accuracy: 1.0000 - val_loss: 0.0129\n",
      "Epoch 50/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9817 - loss: 0.0387 - val_accuracy: 1.0000 - val_loss: 0.0144\n",
      "Epoch 51/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9906 - loss: 0.0317 - val_accuracy: 1.0000 - val_loss: 0.0155\n",
      "Epoch 52/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0232 - val_accuracy: 1.0000 - val_loss: 0.0118\n",
      "Epoch 53/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9842 - loss: 0.0351 - val_accuracy: 1.0000 - val_loss: 0.0185\n",
      "Epoch 54/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9932 - loss: 0.0193 - val_accuracy: 1.0000 - val_loss: 0.0183\n",
      "Epoch 55/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9801 - loss: 0.0474 - val_accuracy: 0.9667 - val_loss: 0.0332\n",
      "Epoch 56/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9736 - loss: 0.0377 - val_accuracy: 1.0000 - val_loss: 0.0220\n",
      "Epoch 57/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9882 - loss: 0.0217 - val_accuracy: 1.0000 - val_loss: 0.0110\n",
      "Epoch 58/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9773 - loss: 0.0419 - val_accuracy: 0.9667 - val_loss: 0.0258\n",
      "Epoch 59/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9875 - loss: 0.0312 - val_accuracy: 1.0000 - val_loss: 0.0326\n",
      "Epoch 60/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9889 - loss: 0.0359 - val_accuracy: 1.0000 - val_loss: 0.0198\n",
      "Epoch 61/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9929 - loss: 0.0335 - val_accuracy: 1.0000 - val_loss: 0.0105\n",
      "Epoch 62/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9842 - loss: 0.0249 - val_accuracy: 1.0000 - val_loss: 0.0123\n",
      "Epoch 63/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9730 - loss: 0.0428 - val_accuracy: 1.0000 - val_loss: 0.0285\n",
      "Epoch 64/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9672 - loss: 0.0481 - val_accuracy: 1.0000 - val_loss: 0.0226\n",
      "Epoch 65/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9929 - loss: 0.0364 - val_accuracy: 1.0000 - val_loss: 0.0454\n",
      "Epoch 66/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9943 - loss: 0.0279 - val_accuracy: 0.9667 - val_loss: 0.0517\n",
      "Epoch 67/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9817 - loss: 0.0246 - val_accuracy: 1.0000 - val_loss: 0.0305\n",
      "Epoch 68/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9800 - loss: 0.0363 - val_accuracy: 1.0000 - val_loss: 0.0166\n",
      "Epoch 69/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9776 - loss: 0.0569 - val_accuracy: 1.0000 - val_loss: 0.0163\n",
      "Epoch 70/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9811 - loss: 0.0300 - val_accuracy: 1.0000 - val_loss: 0.0262\n",
      "Epoch 71/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9811 - loss: 0.0260 - val_accuracy: 1.0000 - val_loss: 0.0220\n",
      "Epoch 72/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9920 - loss: 0.0270 - val_accuracy: 1.0000 - val_loss: 0.0233\n",
      "Epoch 73/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9801 - loss: 0.0494 - val_accuracy: 1.0000 - val_loss: 0.0147\n",
      "Epoch 74/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9906 - loss: 0.0227 - val_accuracy: 1.0000 - val_loss: 0.0137\n",
      "Epoch 75/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9972 - loss: 0.0211 - val_accuracy: 1.0000 - val_loss: 0.0198\n",
      "Epoch 76/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9791 - loss: 0.0327 - val_accuracy: 1.0000 - val_loss: 0.0230\n",
      "Epoch 77/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9871 - loss: 0.0300 - val_accuracy: 0.9667 - val_loss: 0.0377\n",
      "Epoch 78/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9866 - loss: 0.0177 - val_accuracy: 1.0000 - val_loss: 0.0142\n",
      "Epoch 79/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9875 - loss: 0.0172 - val_accuracy: 1.0000 - val_loss: 0.0102\n",
      "Epoch 80/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9929 - loss: 0.0211 - val_accuracy: 1.0000 - val_loss: 0.0171\n",
      "Epoch 81/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9877 - loss: 0.0239 - val_accuracy: 1.0000 - val_loss: 0.0264\n",
      "Epoch 82/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9871 - loss: 0.0205 - val_accuracy: 1.0000 - val_loss: 0.0344\n",
      "Epoch 83/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9929 - loss: 0.0202 - val_accuracy: 0.9667 - val_loss: 0.0508\n",
      "Epoch 84/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9972 - loss: 0.0140 - val_accuracy: 1.0000 - val_loss: 0.0172\n",
      "Epoch 85/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9929 - loss: 0.0226 - val_accuracy: 1.0000 - val_loss: 0.0173\n",
      "Epoch 86/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9960 - loss: 0.0155 - val_accuracy: 1.0000 - val_loss: 0.0143\n",
      "Epoch 87/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9972 - loss: 0.0129 - val_accuracy: 1.0000 - val_loss: 0.0157\n",
      "Epoch 88/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9972 - loss: 0.0157 - val_accuracy: 1.0000 - val_loss: 0.0259\n",
      "Epoch 89/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9730 - loss: 0.0270 - val_accuracy: 1.0000 - val_loss: 0.0159\n",
      "Epoch 90/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9981 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 0.0187\n",
      "Epoch 91/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0128 - val_accuracy: 1.0000 - val_loss: 0.0122\n",
      "Epoch 92/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9871 - loss: 0.0182 - val_accuracy: 1.0000 - val_loss: 0.0196\n",
      "Epoch 93/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9801 - loss: 0.0288 - val_accuracy: 1.0000 - val_loss: 0.0227\n",
      "Epoch 94/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9929 - loss: 0.0245 - val_accuracy: 1.0000 - val_loss: 0.0217\n",
      "Epoch 95/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0145 - val_accuracy: 1.0000 - val_loss: 0.0168\n",
      "Epoch 96/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9871 - loss: 0.0183 - val_accuracy: 1.0000 - val_loss: 0.0174\n",
      "Epoch 97/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0164 - val_accuracy: 1.0000 - val_loss: 0.0277\n",
      "Epoch 98/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0123 - val_accuracy: 1.0000 - val_loss: 0.0138\n",
      "Epoch 99/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0122 - val_accuracy: 0.9667 - val_loss: 0.0323\n",
      "Epoch 100/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9946 - loss: 0.0126 - val_accuracy: 0.9667 - val_loss: 0.0823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b7506d7710>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ Define the MLP model in Keras with Gradient Clipping\n",
    "keras_model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation='relu', input_shape=(4,)),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# ✅ Define the optimizer with Gradient Clipping\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01, clipnorm=1.0)  # Clip gradient norm\n",
    "\n",
    "# ✅ Compile the model\n",
    "keras_model.compile(optimizer=optimizer, \n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# ✅ Train the model\n",
    "keras_model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=1, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 3. Learning Rate Scheduling\n",
    "\n",
    "### 🔹 Why Use Learning Rate Scheduling?\n",
    "- **Adjusts learning rate dynamically** to **improve convergence**  \n",
    "- **Speeds up training** while avoiding overshooting  \n",
    "\n",
    "### 🔹 Common Learning Rate Schedulers\n",
    "\n",
    "#### **1. Exponential Decay**\n",
    "Gradually reduces the learning rate:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\alpha_0 \\times e^{-kt}\n",
    "$$\n",
    "\n",
    "#### **2. Step Decay**\n",
    "Reduces the learning rate **after a fixed number of steps**:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\alpha_0 \\times 0.5^{t//k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.0018, Val Loss: 0.0075, LR: 0.010000\n",
      "Epoch 20, Train Loss: 0.0020, Val Loss: 0.0077, LR: 0.002500\n",
      "Epoch 30, Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.000625\n",
      "Epoch 40, Train Loss: 0.0003, Val Loss: 0.0064, LR: 0.000313\n",
      "Epoch 50, Train Loss: 0.0157, Val Loss: 0.0063, LR: 0.000078\n",
      "Epoch 60, Train Loss: 0.0397, Val Loss: 0.0063, LR: 0.000020\n",
      "Epoch 70, Train Loss: 0.0006, Val Loss: 0.0063, LR: 0.000010\n",
      "Epoch 80, Train Loss: 0.0016, Val Loss: 0.0063, LR: 0.000002\n",
      "Epoch 90, Train Loss: 0.3024, Val Loss: 0.0063, LR: 0.000001\n",
      "Epoch 100, Train Loss: 0.0335, Val Loss: 0.0063, LR: 0.000000\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ✅ Learning rate scheduler (Reduce LR on Plateau)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# ✅ Training loop with LR scheduler\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ✅ Compute validation loss before updating LR\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss += criterion(y_val_pred, y_val).item()\n",
    "\n",
    "    val_loss /= len(test_loader)  # ✅ Compute average validation loss\n",
    "\n",
    "    # ✅ Adjust learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # ✅ Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0187 - val_accuracy: 0.9667 - val_loss: 0.0627 - learning_rate: 0.0100\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9946 - loss: 0.0134 - val_accuracy: 0.9667 - val_loss: 0.0708 - learning_rate: 0.0100\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0157 - val_accuracy: 0.9667 - val_loss: 0.0563 - learning_rate: 0.0100\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0139 - val_accuracy: 0.9667 - val_loss: 0.0362 - learning_rate: 0.0100\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9972 - loss: 0.0117 - val_accuracy: 0.9667 - val_loss: 0.0628 - learning_rate: 0.0100\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0081 - val_accuracy: 0.9667 - val_loss: 0.0665 - learning_rate: 0.0100\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.9667 - val_loss: 0.0397 - learning_rate: 0.0100\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.9667 - val_loss: 0.0489 - learning_rate: 0.0100\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0146 - val_accuracy: 0.9667 - val_loss: 0.0353 - learning_rate: 0.0100\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9972 - loss: 0.0070 - val_accuracy: 0.9667 - val_loss: 0.0548 - learning_rate: 0.0100\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0115 - val_accuracy: 0.9667 - val_loss: 0.0826 - learning_rate: 0.0100\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0096 - val_accuracy: 1.0000 - val_loss: 0.0242 - learning_rate: 0.0100\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9946 - loss: 0.0098 - val_accuracy: 0.9667 - val_loss: 0.0672 - learning_rate: 0.0100\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 0.9667 - val_loss: 0.0622 - learning_rate: 0.0100\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0102 - val_accuracy: 0.9667 - val_loss: 0.0458 - learning_rate: 0.0100\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.9667 - val_loss: 0.0590 - learning_rate: 0.0100\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.9667 - val_loss: 0.0990 - learning_rate: 0.0100\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0125 - val_accuracy: 0.9667 - val_loss: 0.1096 - learning_rate: 0.0050\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.9667 - val_loss: 0.0941 - learning_rate: 0.0050\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0077 - val_accuracy: 0.9667 - val_loss: 0.0825 - learning_rate: 0.0050\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.9667 - val_loss: 0.0903 - learning_rate: 0.0050\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 0.9667 - val_loss: 0.1281 - learning_rate: 0.0050\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.9667 - val_loss: 0.1178 - learning_rate: 0.0025\n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 0.9667 - val_loss: 0.1026 - learning_rate: 0.0025\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 0.9667 - val_loss: 0.1029 - learning_rate: 0.0025\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0085 - val_accuracy: 0.9667 - val_loss: 0.0949 - learning_rate: 0.0025\n",
      "Epoch 27/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.9667 - val_loss: 0.0937 - learning_rate: 0.0025\n",
      "Epoch 28/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.9667 - val_loss: 0.0939 - learning_rate: 0.0012\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.9667 - val_loss: 0.0937 - learning_rate: 0.0012\n",
      "Epoch 30/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0116 - val_accuracy: 0.9667 - val_loss: 0.0955 - learning_rate: 0.0012\n",
      "Epoch 31/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 0.9667 - val_loss: 0.0939 - learning_rate: 0.0012\n",
      "Epoch 32/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.9667 - val_loss: 0.0935 - learning_rate: 0.0012\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0071 - val_accuracy: 0.9667 - val_loss: 0.0942 - learning_rate: 6.2500e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.9667 - val_loss: 0.0944 - learning_rate: 6.2500e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 0.9667 - val_loss: 0.0941 - learning_rate: 6.2500e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.9667 - val_loss: 0.0944 - learning_rate: 6.2500e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 6.2500e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.9667 - val_loss: 0.0946 - learning_rate: 3.1250e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.9667 - val_loss: 0.0950 - learning_rate: 3.1250e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.9667 - val_loss: 0.0944 - learning_rate: 3.1250e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.9667 - val_loss: 0.0937 - learning_rate: 3.1250e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.9667 - val_loss: 0.0947 - learning_rate: 3.1250e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 0.9667 - val_loss: 0.0950 - learning_rate: 1.5625e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.9667 - val_loss: 0.0953 - learning_rate: 1.5625e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0093 - val_accuracy: 0.9667 - val_loss: 0.0953 - learning_rate: 1.5625e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 0.9667 - val_loss: 0.0950 - learning_rate: 1.5625e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0100 - val_accuracy: 0.9667 - val_loss: 0.0952 - learning_rate: 1.5625e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0071 - val_accuracy: 0.9667 - val_loss: 0.0951 - learning_rate: 7.8125e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0063 - val_accuracy: 0.9667 - val_loss: 0.0950 - learning_rate: 7.8125e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0063 - val_accuracy: 0.9667 - val_loss: 0.0949 - learning_rate: 7.8125e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.9667 - val_loss: 0.0951 - learning_rate: 7.8125e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0083 - val_accuracy: 0.9667 - val_loss: 0.0949 - learning_rate: 7.8125e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0061 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 3.9062e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 3.9062e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 0.9667 - val_loss: 0.0947 - learning_rate: 3.9062e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 3.9062e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 3.9062e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 0.9667 - val_loss: 0.0947 - learning_rate: 1.9531e-05\n",
      "Epoch 59/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.9667 - val_loss: 0.0947 - learning_rate: 1.9531e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0068 - val_accuracy: 0.9667 - val_loss: 0.0947 - learning_rate: 1.9531e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0067 - val_accuracy: 0.9667 - val_loss: 0.0947 - learning_rate: 1.9531e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0091 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 1.9531e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0101 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 9.7656e-06\n",
      "Epoch 64/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 9.7656e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.9667 - val_loss: 0.0947 - learning_rate: 9.7656e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 9.7656e-06\n",
      "Epoch 67/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0063 - val_accuracy: 0.9667 - val_loss: 0.0947 - learning_rate: 9.7656e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.9667 - val_loss: 0.0947 - learning_rate: 4.8828e-06\n",
      "Epoch 69/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 4.8828e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 4.8828e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 4.8828e-06\n",
      "Epoch 72/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 4.8828e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 2.4414e-06\n",
      "Epoch 74/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 2.4414e-06\n",
      "Epoch 75/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 2.4414e-06\n",
      "Epoch 76/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0083 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 2.4414e-06\n",
      "Epoch 77/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 2.4414e-06\n",
      "Epoch 78/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 1.2207e-06\n",
      "Epoch 79/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 1.2207e-06\n",
      "Epoch 80/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 1.2207e-06\n",
      "Epoch 81/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0072 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 1.2207e-06\n",
      "Epoch 82/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 1.2207e-06\n",
      "Epoch 83/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 6.1035e-07\n",
      "Epoch 84/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 6.1035e-07\n",
      "Epoch 85/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 6.1035e-07\n",
      "Epoch 86/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 6.1035e-07\n",
      "Epoch 87/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 6.1035e-07\n",
      "Epoch 88/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 3.0518e-07\n",
      "Epoch 89/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 3.0518e-07\n",
      "Epoch 90/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 3.0518e-07\n",
      "Epoch 91/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 3.0518e-07\n",
      "Epoch 92/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0083 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 3.0518e-07\n",
      "Epoch 93/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 1.5259e-07\n",
      "Epoch 94/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0084 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 1.5259e-07\n",
      "Epoch 95/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 1.5259e-07\n",
      "Epoch 96/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 1.5259e-07\n",
      "Epoch 97/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 1.5259e-07\n",
      "Epoch 98/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 7.6294e-08\n",
      "Epoch 99/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 7.6294e-08\n",
      "Epoch 100/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.9667 - val_loss: 0.0948 - learning_rate: 7.6294e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b754642180>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ Learning rate scheduler (Reduce LR on Plateau)\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# ✅ Train the model with the scheduler\n",
    "keras_model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=1, validation_data=(X_test, y_test), callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 10. Interview Questions\n",
    "\n",
    "### 🔹 **Dropout Questions**\n",
    "1️⃣ **Why does Dropout help prevent overfitting?**  \n",
    "   - Forces the network to learn redundant representations.  \n",
    "   \n",
    "2️⃣ **How does Dropout affect training vs. inference?**  \n",
    "   - During training, neurons are randomly dropped with probability **p**.  \n",
    "   - During inference, all neurons are active, but their outputs are scaled by **(1 - p)** to maintain consistency.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Batch Normalization Questions**\n",
    "3️⃣ **Why does BatchNorm improve training?**  \n",
    "   - It stabilizes learning, reduces internal covariate shift, and allows for higher learning rates.  \n",
    "\n",
    "4️⃣ **How does BatchNorm act as a regularization technique?**  \n",
    "   - It introduces slight noise to the activations, similar to Dropout, which reduces overfitting.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Optimization Questions**\n",
    "5️⃣ **Why use Xavier (Glorot) initialization?**  \n",
    "   - It ensures that the variance of activations remains stable across layers, preventing vanishing/exploding gradients.  \n",
    "\n",
    "6️⃣ **Why use He initialization for ReLU networks?**  \n",
    "   - He initialization scales weights to maintain variance in ReLU-based networks, improving convergence.  \n",
    "\n",
    "7️⃣ **What is the purpose of Gradient Clipping?**  \n",
    "   - It prevents exploding gradients by capping their magnitude during backpropagation.  \n",
    "\n",
    "8️⃣ **When should you use Learning Rate Scheduling?**  \n",
    "   - When training loss plateaus, reducing the learning rate helps the model converge better.  \n",
    "\n",
    "9️⃣ **What are the different types of Learning Rate Schedulers?**  \n",
    "   - **Exponential Decay:** Decreases learning rate exponentially over time.  \n",
    "   - **Step Decay:** Reduces learning rate after fixed steps.  \n",
    "   - **Adaptive Methods (Adam, RMSprop):** Adjusts learning rates dynamically based on past gradients.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Weight Initialization Questions**\n",
    "1️⃣ **Why is weight initialization important?**  \n",
    "   - Prevents vanishing/exploding gradients.  \n",
    "\n",
    "2️⃣ **When should you use Xavier initialization?**  \n",
    "   - For **sigmoid and tanh** activation functions.  \n",
    "\n",
    "3️⃣ **When should you use He initialization?**  \n",
    "   - For **ReLU and Leaky ReLU** activations.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Gradient Clipping Questions**\n",
    "4️⃣ **Why do we clip gradients?**  \n",
    "   - To prevent **exploding gradients**, especially in deep networks and RNNs.  \n",
    "\n",
    "5️⃣ **What is the difference between `clipnorm` and `clipvalue`?**  \n",
    "   - **`clipnorm`** rescales gradients if their **L2 norm exceeds a threshold**.  \n",
    "   - **`clipvalue`** clips each gradient component **individually**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 **Learning Rate Scheduling Questions**\n",
    "6️⃣ **Why use a dynamic learning rate instead of a fixed one?**  \n",
    "   - **Higher learning rates** speed up training initially.  \n",
    "   - **Lower learning rates** improve convergence at later stages.  \n",
    "\n",
    "7️⃣ **What’s the difference between Exponential Decay and Step Decay?**  \n",
    "   - **Exponential Decay** gradually decreases learning rate every step.  \n",
    "   - **Step Decay** reduces learning rate **at fixed intervals**.  \n",
    "\n",
    "8️⃣ **When should you decay the learning rate?**  \n",
    "   - When the loss **plateaus** and is no longer improving.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
