{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ **Transformers from Scratch - A Complete Guide**\n",
    "\n",
    "This notebook provides a **detailed breakdown** of Transformer models from a **pure mathematical and coding perspective**. We will cover:\n",
    "\n",
    "âœ” **Mathematical foundations of self-attention**  \n",
    "âœ” **Multi-head attention, positional encoding, and feedforward layers**  \n",
    "âœ” **How to build a Transformer step-by-step (without external libraries like Hugging Face)**  \n",
    "âœ” **Pure PyTorch and Keras implementations**  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **1. Introduction to Transformers**\n",
    "Traditional **Recurrent Neural Networks (RNNs)** process words **sequentially**, making them **slow and difficult to parallelize**.  \n",
    "Transformers **solve this problem** by using **self-attention**, which allows them to process all words **at the same time**.\n",
    "\n",
    "âœ” **Process entire sequences in parallel** (unlike RNNs)  \n",
    "âœ” **Capture long-range dependencies efficiently**  \n",
    "âœ” **Used in models like GPT, BERT, and T5**  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§® **2. Mathematical Foundation of Transformers**\n",
    "\n",
    "### ğŸ”¹ **1. The Self-Attention Mechanism**\n",
    "Self-attention is the **core of Transformers**. It allows the model to weigh different words **based on their relevance** to the current word.\n",
    "\n",
    "#### **Step 1: Compute Query, Key, and Value Matrices**\n",
    "Each word in the sentence is **transformed into three vectors**:\n",
    "\n",
    "$$\n",
    "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- **\\( X \\)** = input sentence as word embeddings  \n",
    "- **\\( W_Q, W_K, W_V \\)** = weight matrices for **Query, Key, and Value**  \n",
    "\n",
    "#### **Step 2: Compute Attention Scores**\n",
    "We calculate the similarity between each word using **dot-product attention**:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- **\\( d_k \\)** = embedding size (used for scaling)  \n",
    "- **Softmax** ensures attention weights sum to **1**.  \n",
    "\n",
    "This means that **important words get more weight** while unimportant words contribute less.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ **2. Multi-Head Attention**\n",
    "Instead of a **single attention head**, Transformers use **multiple attention heads** to focus on **different parts** of the sentence.\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W_O\n",
    "$$\n",
    "\n",
    "Each **head** applies independent self-attention, helping the model capture different **semantic relationships**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ **3. Positional Encoding**\n",
    "Transformers **do not have recurrence**, so they need a way to **encode word order**.  \n",
    "Positional Encoding is added to the embeddings:\n",
    "\n",
    "$$\n",
    "PE(i, 2j) = \\sin \\left(\\frac{i}{10000^{2j/d}}\\right), \\quad PE(i, 2j+1) = \\cos \\left(\\frac{i}{10000^{2j/d}}\\right)\n",
    "$$\n",
    "\n",
    "where **\\( i \\)** is the word position and **\\( j \\)** is the embedding index.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ **4. Feedforward Neural Network (FFN)**\n",
    "Each Transformer layer also includes a **fully connected network**:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "\n",
    "This helps the model **learn complex patterns** beyond attention.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ **3. Implementing a Transformer from Scratch (PyTorch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Import basic PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embedding): Embedding(10000, 128)\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-1): 2 x TransformerEncoderLayer(\n",
      "      (self_attention): SelfAttention(\n",
      "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=10000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# âœ… 1. Define Self-Attention Mechanism\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Query, Key, and Value weight matrices\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, embed_dim = x.shape\n",
    "        \n",
    "        # Compute Query, Key, Value\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # Compute Attention Scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return self.W_o(output)\n",
    "\n",
    "# âœ… 2. Define Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.self_attention = SelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-Attention + Residual Connection\n",
    "        attn_output = self.self_attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "\n",
    "        # Feedforward + Residual Connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "# âœ… 3. Define Full Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=2, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 500, embed_dim))  # Fixed max length = 500\n",
    "        self.encoder_layers = nn.ModuleList([TransformerEncoderLayer(embed_dim, num_heads, hidden_dim) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# âœ… 4. Initialize and Print Model Summary\n",
    "vocab_size = 10000  # Example vocab size\n",
    "model = Transformer(vocab_size)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ **3. Implementing a Transformer from Scratch (PyTorch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ transformer_encoder_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">330,240</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290,000</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚     \u001b[38;5;34m1,280,000\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ transformer_encoder_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚       \u001b[38;5;34m330,240\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m10000\u001b[0m)     â”‚     \u001b[38;5;34m1,290,000\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,900,240</span> (11.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,900,240\u001b[0m (11.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,900,240</span> (11.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,900,240\u001b[0m (11.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# âœ… Define Multi-Head Attention\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.att(x, x)\n",
    "\n",
    "# âœ… Define Transformer Encoder Layer\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(hidden_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization()\n",
    "        self.layernorm2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.layernorm1(x + self.attention(x))\n",
    "        x = self.layernorm2(x + self.ffn(x))\n",
    "        return x\n",
    "\n",
    "# âœ… Initialize Model\n",
    "inputs = layers.Input(shape=(500,))\n",
    "x = layers.Embedding(10000, 128)(inputs)\n",
    "x = TransformerEncoder(128, 4, 256)(x)\n",
    "x = layers.Dense(10000, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs, x)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â“ **Key Questions for Interviews**\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ **What is the main advantage of Transformers over RNNs and LSTMs?**  \n",
    "Transformers process **entire sequences in parallel**, unlike RNNs, which process text **sequentially**. This enables **faster training** and helps capture **long-range dependencies** better.\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **How does the self-attention mechanism work?**  \n",
    "Self-attention allows the model to determine the **importance of each word** in a sentence relative to others. It computes:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "where \\( Q, K, V \\) are **Query, Key, and Value matrices** derived from input embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **Why do we scale the dot product by \\( \\sqrt{d_k} \\) in self-attention?**  \n",
    "Without scaling, the dot product of **large vectors** can produce **very high values**, leading to extreme softmax outputs (close to 0 or 1). Scaling by \\( \\sqrt{d_k} \\) ensures **stability and smooth gradients**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ **What is multi-head attention and why is it important?**  \n",
    "Multi-head attention allows the model to **focus on multiple aspects** of a sentence **simultaneously**. Instead of using a single attention mechanism, we apply multiple **independent attention heads** and concatenate their results:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W_O\n",
    "$$\n",
    "\n",
    "This helps the model capture **different relationships** within the sentence.\n",
    "\n",
    "---\n",
    "\n",
    "### 5ï¸âƒ£ **Why do Transformers use positional encoding?**  \n",
    "Transformers **do not have recurrence** (unlike RNNs), so they need **positional encodings** to retain word order. The encoding function is:\n",
    "\n",
    "$$\n",
    "PE(i, 2j) = \\sin \\left(\\frac{i}{10000^{2j/d}}\\right), \\quad PE(i, 2j+1) = \\cos \\left(\\frac{i}{10000^{2j/d}}\\right)\n",
    "$$\n",
    "\n",
    "where \\( i \\) is the position and \\( j \\) is the embedding dimension.\n",
    "\n",
    "---\n",
    "\n",
    "### 6ï¸âƒ£ **What are LayerNorm and residual connections used for in Transformers?**  \n",
    "âœ” **LayerNorm** stabilizes training by normalizing activations across feature dimensions.  \n",
    "âœ” **Residual connections** help **prevent vanishing gradients** by allowing direct information flow:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{LayerNorm}(x + \\text{Self-Attention}(x))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 7ï¸âƒ£ **What is the Feedforward Neural Network (FFN) in a Transformer?**  \n",
    "Each Transformer layer contains a **fully connected feedforward network (FFN)** after the attention mechanism:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "\n",
    "This introduces **non-linearity** and allows the model to learn more **complex representations**.\n",
    "\n",
    "---\n",
    "\n",
    "### 8ï¸âƒ£ **How do Transformers handle long sequences efficiently?**  \n",
    "Transformers use **self-attention**, which has a computational complexity of **\\( O(n^2) \\)**. To handle long sequences, models like **Longformer and Reformer** use **sparse attention**, reducing complexity to **\\( O(n) \\)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 9ï¸âƒ£ **What is the difference between an Encoder and a Decoder in a Transformer?**  \n",
    "âœ” **Encoder** â†’ Processes input sequences and extracts meaningful representations.  \n",
    "âœ” **Decoder** â†’ Generates output sequences (used in translation, text generation).  \n",
    "âœ” **Both** use **self-attention**, but **decoders** also have **cross-attention**, allowing them to attend to encoder outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”Ÿ **What are the main differences between BERT and GPT?**  \n",
    "| Feature | BERT | GPT |\n",
    "|---------|------|-----|\n",
    "| Architecture | Bidirectional | Unidirectional |\n",
    "| Pretraining Task | Masked Language Model (MLM) | Causal Language Model (CLM) |\n",
    "| Use Case | Text Understanding | Text Generation |\n",
    "\n",
    "BERT **understands** text by predicting missing words, while GPT **generates** text based on previous tokens.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
